{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features:\n",
    "1. Fraction of clauses that are unit clauses. <!-- exactly one literal -->\n",
    "2. Fraction of clauses that are Horn clauses. <!-- at most one non-negated literal -->\n",
    "3. Fraction of clauses that are ground Clauses. <!-- ? -->\n",
    "4. Fraction of clauses that are demodulators. <!-- equality used as rule to rewrite newly inferred clause -->\n",
    "5. Fraction of clauses that are rewrite rules (oriented demodulators). <!-- ? -->\n",
    "6. Fraction of clauses that are purely positive.\n",
    "7. Fraction of clauses that are purely negative.\n",
    "8. Fraction of clauses that are mixed positive and negative.\n",
    "9. Maximum clause length. <!-- number of literals -->\n",
    "10. Average clause length.\n",
    "11. Maximum clause depth. <!-- see below -->\n",
    "12. Average clause depth.\n",
    "13. Maximum clause weight. <!-- defined by prover; probably its symbol count, excluding commas, parentheses, negation symbols, and disjunction symbols -->\n",
    "14. Average clause weight.\n",
    "\n",
    "<!-- \n",
    "Depth of Term, Atom, Literal, Clause\n",
    "* depth of variable, constant, or propositional atom: 0;\n",
    "* depth of term or atom with arguments: one more than the maximum argument depth;\n",
    "* depth of literal: depth of its atom (negation signs don't count);\n",
    "* depth of clause: maximum of depths of literals;\n",
    "* For example, p(x) | -p(f(x)) has depth 2.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73684</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73872</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77107</td>\n",
       "      <td>0.068363</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74436</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74060</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72932</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.080827</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73308</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3   4        5         6        7   8   \\\n",
       "0  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "1  0.83307  0.99682  0.83307  0.76948   0  0.77107  0.068363  0.16057   6   \n",
       "2  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "3  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "4  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "\n",
       "       9    ...         48       49       50        51       52      53  \\\n",
       "0  1.2734   ...    0.73684  0.00188  0.73872  0.073308  0.18797 -100.00   \n",
       "1  1.2734   ...    0.74248  0.00188  0.74436  0.067669  0.18797    0.08   \n",
       "2  1.2734   ...    0.74060  0.00188  0.74248  0.069549  0.18797 -100.00   \n",
       "3  1.2734   ...    0.72932  0.00188  0.73120  0.080827  0.18797 -100.00   \n",
       "4  1.2734   ...    0.73120  0.00188  0.73308  0.078947  0.18797 -100.00   \n",
       "\n",
       "       54     55      56      57  \n",
       "0 -100.00 -100.0 -100.00 -100.00  \n",
       "1    0.08    0.2    0.08    0.08  \n",
       "2 -100.00 -100.0 -100.00 -100.00  \n",
       "3 -100.00 -100.0 -100.00 -100.00  \n",
       "4 -100.00 -100.0 -100.00 -100.00  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/all-data-raw.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       53      54     55      56      57\n",
      "0 -100.00 -100.00 -100.0 -100.00 -100.00\n",
      "1    0.08    0.08    0.2    0.08    0.08\n",
      "2 -100.00 -100.00 -100.0 -100.00 -100.00\n",
      "3 -100.00 -100.00 -100.0 -100.00 -100.00\n",
      "4 -100.00 -100.00 -100.0 -100.00 -100.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>heuristic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.73684</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73872</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77107</td>\n",
       "      <td>0.068363</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74436</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.74060</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.72932</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.080827</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73308</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2        3  4        5         6        7  8  \\\n",
       "0  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "1  0.83307  0.99682  0.83307  0.76948  0  0.77107  0.068363  0.16057  6   \n",
       "2  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "3  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "4  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "\n",
       "        9    ...            44       45       46       47       48       49  \\\n",
       "0  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.73684  0.00188   \n",
       "1  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.74248  0.00188   \n",
       "2  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.74060  0.00188   \n",
       "3  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.72932  0.00188   \n",
       "4  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.73120  0.00188   \n",
       "\n",
       "        50        51       52  heuristic  \n",
       "0  0.73872  0.073308  0.18797          0  \n",
       "1  0.74436  0.067669  0.18797          1  \n",
       "2  0.74248  0.069549  0.18797          0  \n",
       "3  0.73120  0.080827  0.18797          0  \n",
       "4  0.73308  0.078947  0.18797          0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_times = df.iloc[:, -5:]\n",
    "print(h_times.head())\n",
    "\n",
    "import numpy as np\n",
    "def best_heuristic(row):\n",
    "    n_heuristics = 5\n",
    "    h_times = row[-n_heuristics:].reset_index(drop=True)\n",
    "    h_times.replace({-100.0 : np.nan}, inplace=True)\n",
    "    idx, min_time = h_times.idxmin(), h_times.min()\n",
    "    if np.isnan(min_time):\n",
    "       return 0\n",
    "    else:\n",
    "       return idx+1\n",
    "\n",
    "df['heuristic'] = df.apply(best_heuristic, axis=1)\n",
    "df.drop([53, 54, 55, 56, 57], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2554\n",
       "1    1089\n",
       "3     748\n",
       "5     624\n",
       "4     617\n",
       "2     486\n",
       "Name: heuristic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['heuristic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   49.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 12, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X, y = df.drop(['heuristic'], axis=1).astype('float64'), df['heuristic']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=44)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__n_neighbors': range(1,15),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "knn_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "knn_grid.fit(X, y)\n",
    "\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 12, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.6036\n"
     ]
    }
   ],
   "source": [
    "print(knn_grid.best_params_)\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.1100785 , 0.03610344, 0.03349228, 0.03313601, 0.03274267,\n",
       "        0.03311157, 0.03241754, 0.03190088, 0.03241916, 0.03449843,\n",
       "        0.03517005, 0.03594844, 0.03356011, 0.03292785, 0.03286555,\n",
       "        0.03289592, 0.0325963 , 0.03850722, 0.03231304, 0.03217304,\n",
       "        0.03228445, 0.03587463, 0.03352423, 0.0330765 , 0.044292  ,\n",
       "        0.03924787, 0.03848658, 0.04546318]),\n",
       " 'std_fit_time': array([0.09040997, 0.00889914, 0.00270994, 0.00304311, 0.00239372,\n",
       "        0.00203112, 0.00105066, 0.0016699 , 0.00127894, 0.00422062,\n",
       "        0.00164387, 0.00651597, 0.0019609 , 0.00150714, 0.00197402,\n",
       "        0.00046716, 0.001533  , 0.00865069, 0.00134937, 0.00154251,\n",
       "        0.00224909, 0.01138787, 0.00272046, 0.0023571 , 0.01361918,\n",
       "        0.00846211, 0.01015233, 0.01232112]),\n",
       " 'mean_score_time': array([0.08051262, 0.06128943, 0.08200164, 0.08054371, 0.09369106,\n",
       "        0.08880317, 0.09941392, 0.09473569, 0.10661414, 0.11513705,\n",
       "        0.13452382, 0.11915903, 0.11400561, 0.11467247, 0.1172421 ,\n",
       "        0.11965005, 0.12199395, 0.12677186, 0.12373662, 0.12404256,\n",
       "        0.12820451, 0.12850702, 0.13015432, 0.13405614, 0.16689405,\n",
       "        0.16908071, 0.15148435, 0.1752099 ]),\n",
       " 'std_score_time': array([0.01683343, 0.00403684, 0.00411221, 0.00748498, 0.00902838,\n",
       "        0.00438707, 0.00592829, 0.00370278, 0.00687805, 0.01497032,\n",
       "        0.02397557, 0.0242831 , 0.006288  , 0.0052671 , 0.00721389,\n",
       "        0.00740111, 0.00378012, 0.00924644, 0.00431699, 0.00351182,\n",
       "        0.00428909, 0.0082693 , 0.00200013, 0.00785974, 0.03216576,\n",
       "        0.04224064, 0.02312492, 0.02639059]),\n",
       " 'param_classifier__n_neighbors': masked_array(data=[1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9,\n",
       "                    10, 10, 11, 11, 12, 12, 13, 13, 14, 14],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__n_neighbors': 1, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 1, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 2, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 2, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 3, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 4, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 4, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 5, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 5, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 6, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 6, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 7, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 8, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 8, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 9, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 9, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 10, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 10, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 11, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 11, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 12, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 12, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 13, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 13, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 14, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 14, 'classifier__weights': 'distance'}],\n",
       " 'split0_test_score': array([0.58794788, 0.58794788, 0.58469055, 0.58957655, 0.56840391,\n",
       "        0.59609121, 0.55863192, 0.59771987, 0.56188925, 0.59771987,\n",
       "        0.57166124, 0.61237785, 0.58143322, 0.60586319, 0.57003257,\n",
       "        0.60749186, 0.57654723, 0.61400651, 0.57491857, 0.61237785,\n",
       "        0.58143322, 0.61237785, 0.5732899 , 0.61563518, 0.56677524,\n",
       "        0.61563518, 0.56351792, 0.61400651]),\n",
       " 'split1_test_score': array([0.59283388, 0.59283388, 0.56514658, 0.59771987, 0.55211726,\n",
       "        0.59771987, 0.56840391, 0.6009772 , 0.56188925, 0.6009772 ,\n",
       "        0.54397394, 0.59609121, 0.54885993, 0.59771987, 0.54885993,\n",
       "        0.60260586, 0.53583062, 0.59120521, 0.53583062, 0.59446254,\n",
       "        0.54397394, 0.58957655, 0.53908795, 0.59283388, 0.52442997,\n",
       "        0.58631922, 0.53094463, 0.58469055]),\n",
       " 'split2_test_score': array([0.58794788, 0.58794788, 0.57654723, 0.58794788, 0.5781759 ,\n",
       "        0.59446254, 0.56026059, 0.58631922, 0.54723127, 0.59120521,\n",
       "        0.55211726, 0.59446254, 0.54397394, 0.58631922, 0.55863192,\n",
       "        0.58631922, 0.56677524, 0.59771987, 0.56514658, 0.59771987,\n",
       "        0.5504886 , 0.59120521, 0.54397394, 0.6009772 , 0.54397394,\n",
       "        0.59609121, 0.54560261, 0.59934853]),\n",
       " 'split3_test_score': array([0.59120521, 0.59120521, 0.59283388, 0.58469055, 0.56188925,\n",
       "        0.60586319, 0.56514658, 0.59609121, 0.54723127, 0.59120521,\n",
       "        0.54234528, 0.6009772 , 0.53908795, 0.59446254, 0.5504886 ,\n",
       "        0.58469055, 0.56026059, 0.59609121, 0.5504886 , 0.58794788,\n",
       "        0.55863192, 0.6009772 , 0.54723127, 0.59609121, 0.54397394,\n",
       "        0.6009772 , 0.53094463, 0.58957655]),\n",
       " 'split4_test_score': array([0.55228758, 0.55228758, 0.54248366, 0.55555556, 0.54084967,\n",
       "        0.55882353, 0.53921569, 0.55882353, 0.55392157, 0.56372549,\n",
       "        0.55392157, 0.57189542, 0.54575163, 0.57026144, 0.5620915 ,\n",
       "        0.56862745, 0.55228758, 0.57026144, 0.54248366, 0.56535948,\n",
       "        0.54575163, 0.56372549, 0.54575163, 0.57026144, 0.54901961,\n",
       "        0.57189542, 0.55228758, 0.57189542]),\n",
       " 'split5_test_score': array([0.61111111, 0.61111111, 0.60130719, 0.60784314, 0.57679739,\n",
       "        0.60784314, 0.56045752, 0.59803922, 0.58660131, 0.60947712,\n",
       "        0.57352941, 0.61928105, 0.56699346, 0.61764706, 0.56862745,\n",
       "        0.61437908, 0.55555556, 0.61601307, 0.55392157, 0.60784314,\n",
       "        0.54575163, 0.61111111, 0.54738562, 0.60457516, 0.54411765,\n",
       "        0.60457516, 0.53431373, 0.60620915]),\n",
       " 'split6_test_score': array([0.58265139, 0.58265139, 0.58592471, 0.58265139, 0.58919804,\n",
       "        0.60392799, 0.57283142, 0.60883797, 0.57283142, 0.59574468,\n",
       "        0.57446809, 0.59574468, 0.58756137, 0.60065466, 0.59247136,\n",
       "        0.599018  , 0.58265139, 0.60392799, 0.5695581 , 0.60392799,\n",
       "        0.58101473, 0.60883797, 0.57610475, 0.6202946 , 0.56628478,\n",
       "        0.61374795, 0.5695581 , 0.61538462]),\n",
       " 'split7_test_score': array([0.58852459, 0.58852459, 0.59344262, 0.58688525, 0.58196721,\n",
       "        0.61803279, 0.59016393, 0.60983607, 0.58852459, 0.60819672,\n",
       "        0.58360656, 0.6147541 , 0.56721311, 0.61803279, 0.56885246,\n",
       "        0.60983607, 0.56557377, 0.6147541 , 0.5704918 , 0.61803279,\n",
       "        0.57213115, 0.62622951, 0.5557377 , 0.61803279, 0.5557377 ,\n",
       "        0.62459016, 0.55409836, 0.62459016]),\n",
       " 'split8_test_score': array([0.58784893, 0.58784893, 0.55829228, 0.58949097, 0.55829228,\n",
       "        0.58784893, 0.58128079, 0.59934319, 0.55829228, 0.59277504,\n",
       "        0.55993432, 0.591133  , 0.57142857, 0.59934319, 0.56978654,\n",
       "        0.60262726, 0.57307061, 0.60591133, 0.55172414, 0.60426929,\n",
       "        0.54844007, 0.59605911, 0.55665025, 0.60262726, 0.56321839,\n",
       "        0.60098522, 0.56157635, 0.59605911]),\n",
       " 'split9_test_score': array([0.60690789, 0.60690789, 0.58388158, 0.62335526, 0.58388158,\n",
       "        0.62828947, 0.5625    , 0.60526316, 0.56743421, 0.61677632,\n",
       "        0.55427632, 0.62006579, 0.55592105, 0.61842105, 0.56085526,\n",
       "        0.61677632, 0.53453947, 0.62171053, 0.54769737, 0.62335526,\n",
       "        0.55427632, 0.62006579, 0.55427632, 0.61513158, 0.54605263,\n",
       "        0.60855263, 0.54769737, 0.61513158]),\n",
       " 'mean_test_score': array([0.58891795, 0.58891795, 0.57845701, 0.59055247, 0.56914024,\n",
       "        0.59986924, 0.5658712 , 0.59610984, 0.56456358, 0.59676365,\n",
       "        0.56096764, 0.60166721, 0.56080418, 0.60084995, 0.56505394,\n",
       "        0.59921543, 0.56031383, 0.60313828, 0.55622753, 0.60150376,\n",
       "        0.55818895, 0.60199412, 0.5539392 , 0.60362864, 0.55034325,\n",
       "        0.60232102, 0.54903563, 0.60166721]),\n",
       " 'std_test_score': array([0.01489828, 0.01489828, 0.01719973, 0.0166261 , 0.01477928,\n",
       "        0.01767817, 0.01309833, 0.01400123, 0.01380033, 0.01370961,\n",
       "        0.01333172, 0.0143128 , 0.01576334, 0.01452448, 0.0116998 ,\n",
       "        0.01433676, 0.01530728, 0.01439933, 0.01244246, 0.01573107,\n",
       "        0.01392185, 0.0170908 , 0.01163321, 0.01434252, 0.0123813 ,\n",
       "        0.01439436, 0.01308103, 0.0155716 ]),\n",
       " 'rank_test_score': array([14, 14, 16, 13, 17,  9, 18, 12, 20, 11, 21,  5, 22,  8, 19, 10, 23,\n",
       "         2, 25,  7, 24,  4, 26,  1, 27,  3, 28,  5], dtype=int32),\n",
       " 'split0_train_score': array([0.97547238, 0.97547238, 0.7916061 , 0.97565407, 0.75345203,\n",
       "        0.97710756, 0.71420785, 0.97747093, 0.68986192, 0.97765262,\n",
       "        0.67296512, 0.97747093, 0.66188227, 0.97765262, 0.64534884,\n",
       "        0.97765262, 0.62899709, 0.97765262, 0.62191134, 0.97765262,\n",
       "        0.61518895, 0.97765262, 0.60701308, 0.97765262, 0.60483285,\n",
       "        0.97765262, 0.60010901, 0.97765262]),\n",
       " 'split1_train_score': array([0.97583576, 0.97583576, 0.78960756, 0.97583576, 0.75436047,\n",
       "        0.97674419, 0.71947674, 0.97710756, 0.69349564, 0.97728924,\n",
       "        0.67823401, 0.97728924, 0.66115552, 0.97728924, 0.64516715,\n",
       "        0.97728924, 0.63917151, 0.97728924, 0.63135901, 0.97728924,\n",
       "        0.62318314, 0.97728924, 0.61573401, 0.97728924, 0.60919331,\n",
       "        0.97728924, 0.60483285, 0.97728924]),\n",
       " 'split2_train_score': array([0.97801599, 0.97801599, 0.79015262, 0.97710756, 0.75436047,\n",
       "        0.97874273, 0.71311773, 0.97892442, 0.69022529, 0.9791061 ,\n",
       "        0.6744186 , 0.9791061 , 0.66424419, 0.9791061 , 0.65134448,\n",
       "        0.9791061 , 0.64080669, 0.9791061 , 0.63390262, 0.9791061 ,\n",
       "        0.62590843, 0.9791061 , 0.61337209, 0.9791061 , 0.609375  ,\n",
       "        0.9791061 , 0.60773983, 0.9791061 ]),\n",
       " 'split3_train_score': array([0.97638081, 0.97638081, 0.79106105, 0.97710756, 0.75617733,\n",
       "        0.97837936, 0.71547965, 0.97837936, 0.68968023, 0.97819767,\n",
       "        0.67478198, 0.97874273, 0.6627907 , 0.97874273, 0.64698401,\n",
       "        0.97874273, 0.64098837, 0.97874273, 0.63135901, 0.97874273,\n",
       "        0.62281977, 0.97874273, 0.61609738, 0.97874273, 0.60356105,\n",
       "        0.97874273, 0.60337936, 0.97874273]),\n",
       " 'split4_train_score': array([0.97366509, 0.97366509, 0.79113694, 0.97439157, 0.75826371,\n",
       "        0.97693425, 0.7210316 , 0.97711587, 0.69705776, 0.97693425,\n",
       "        0.67617145, 0.97693425, 0.66291319, 0.97711587, 0.65001816,\n",
       "        0.97711587, 0.64111878, 0.97711587, 0.63439884, 0.97711587,\n",
       "        0.62277515, 0.97711587, 0.61732655, 0.97711587, 0.61387577,\n",
       "        0.97711587, 0.60988013, 0.97711587]),\n",
       " 'split5_train_score': array([0.97529967, 0.97529967, 0.78823102, 0.97529967, 0.75118053,\n",
       "        0.97766073, 0.7137668 , 0.97784235, 0.69088267, 0.97784235,\n",
       "        0.67598983, 0.97784235, 0.6594624 , 0.97784235, 0.64820196,\n",
       "        0.97784235, 0.63476208, 0.97784235, 0.6294951 , 0.97784235,\n",
       "        0.62549946, 0.97784235, 0.61605521, 0.97784235, 0.61278605,\n",
       "        0.97784235, 0.60806393, 0.97784235]),\n",
       " 'split6_train_score': array([0.97730162, 0.97730162, 0.78954058, 0.97693844, 0.75177047,\n",
       "        0.97802796, 0.71454512, 0.97857273, 0.69311785, 0.97857273,\n",
       "        0.67368803, 0.97857273, 0.65571091, 0.97875431, 0.65008171,\n",
       "        0.97875431, 0.63755221, 0.97875431, 0.62629381, 0.97875431,\n",
       "        0.61739604, 0.97875431, 0.61539858, 0.97875431, 0.60504812,\n",
       "        0.97875431, 0.60159797, 0.97875431]),\n",
       " 'split7_train_score': array([0.97458243, 0.97458243, 0.7899419 , 0.97494553, 0.75363108,\n",
       "        0.97639797, 0.71514161, 0.97676107, 0.68917938, 0.97676107,\n",
       "        0.67392883, 0.97676107, 0.65740741, 0.97694263, 0.64342774,\n",
       "        0.97694263, 0.63271605, 0.97694263, 0.6328976 , 0.97694263,\n",
       "        0.62418301, 0.97694263, 0.61528686, 0.97694263, 0.61401598,\n",
       "        0.97694263, 0.60566449, 0.97694263]),\n",
       " 'split8_train_score': array([0.97567617, 0.97567617, 0.79252133, 0.97476856, 0.75549101,\n",
       "        0.97676529, 0.71827918, 0.97658377, 0.69322926, 0.97658377,\n",
       "        0.67671084, 0.97676529, 0.66418588, 0.97676529, 0.65220548,\n",
       "        0.97676529, 0.64058813, 0.97676529, 0.62770013, 0.97676529,\n",
       "        0.61826103, 0.97676529, 0.61317844, 0.97676529, 0.60864041,\n",
       "        0.97676529, 0.60029043, 0.97676529]),\n",
       " 'split9_train_score': array([0.97186933, 0.97186933, 0.79056261, 0.97586207, 0.75099819,\n",
       "        0.97568058, 0.71161525, 0.97622505, 0.69147005, 0.976951  ,\n",
       "        0.67186933, 0.976951  , 0.6569873 , 0.97713249, 0.64446461,\n",
       "        0.97713249, 0.63575318, 0.97713249, 0.63194192, 0.97713249,\n",
       "        0.62250454, 0.97713249, 0.6154265 , 0.97713249, 0.60816697,\n",
       "        0.97713249, 0.60544465, 0.97713249]),\n",
       " 'mean_train_score': array([0.97540992, 0.97540992, 0.79043617, 0.97579108, 0.75396853,\n",
       "        0.97724406, 0.71566615, 0.97749831, 0.69182001, 0.97758908,\n",
       "        0.6748758 , 0.97764357, 0.66067398, 0.97773436, 0.64772441,\n",
       "        0.97773436, 0.63724541, 0.97773436, 0.63012594, 0.97773436,\n",
       "        0.62177195, 0.97773436, 0.61448887, 0.97773436, 0.60894955,\n",
       "        0.97773436, 0.60470026, 0.97773436]),\n",
       " 'std_train_score': array([0.00166871, 0.00166871, 0.00115283, 0.00093643, 0.00218542,\n",
       "        0.00089873, 0.00283446, 0.00085928, 0.00229874, 0.00079559,\n",
       "        0.00181052, 0.00083297, 0.00294327, 0.00080411, 0.00293343,\n",
       "        0.00080411, 0.0039194 , 0.00080411, 0.00367409, 0.00080411,\n",
       "        0.00340971, 0.00080411, 0.00275389, 0.00080411, 0.00356101,\n",
       "        0.00080411, 0.0031746 , 0.00080411])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "results_knn = pd.DataFrame(knn_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__n_neighbors</th>\n",
       "      <th>param_classifier__weights</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110079</td>\n",
       "      <td>0.090410</td>\n",
       "      <td>0.080513</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978016</td>\n",
       "      <td>0.976381</td>\n",
       "      <td>0.973665</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>0.977302</td>\n",
       "      <td>0.974582</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>0.971869</td>\n",
       "      <td>0.975410</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036103</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.061289</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978016</td>\n",
       "      <td>0.976381</td>\n",
       "      <td>0.973665</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>0.977302</td>\n",
       "      <td>0.974582</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>0.971869</td>\n",
       "      <td>0.975410</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>0.576547</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790153</td>\n",
       "      <td>0.791061</td>\n",
       "      <td>0.791137</td>\n",
       "      <td>0.788231</td>\n",
       "      <td>0.789541</td>\n",
       "      <td>0.789942</td>\n",
       "      <td>0.792521</td>\n",
       "      <td>0.790563</td>\n",
       "      <td>0.790436</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033136</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.080544</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977108</td>\n",
       "      <td>0.977108</td>\n",
       "      <td>0.974392</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>0.974946</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.975862</td>\n",
       "      <td>0.975791</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032743</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.093691</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.568404</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.578176</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754360</td>\n",
       "      <td>0.756177</td>\n",
       "      <td>0.758264</td>\n",
       "      <td>0.751181</td>\n",
       "      <td>0.751770</td>\n",
       "      <td>0.753631</td>\n",
       "      <td>0.755491</td>\n",
       "      <td>0.750998</td>\n",
       "      <td>0.753969</td>\n",
       "      <td>0.002185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.088803</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>0.605863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.978379</td>\n",
       "      <td>0.976934</td>\n",
       "      <td>0.977661</td>\n",
       "      <td>0.978028</td>\n",
       "      <td>0.976398</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.975681</td>\n",
       "      <td>0.977244</td>\n",
       "      <td>0.000899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.032418</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.099414</td>\n",
       "      <td>0.005928</td>\n",
       "      <td>4</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>0.568404</td>\n",
       "      <td>0.560261</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713118</td>\n",
       "      <td>0.715480</td>\n",
       "      <td>0.721032</td>\n",
       "      <td>0.713767</td>\n",
       "      <td>0.714545</td>\n",
       "      <td>0.715142</td>\n",
       "      <td>0.718279</td>\n",
       "      <td>0.711615</td>\n",
       "      <td>0.715666</td>\n",
       "      <td>0.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.031901</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.094736</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>4</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978924</td>\n",
       "      <td>0.978379</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978573</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.976584</td>\n",
       "      <td>0.976225</td>\n",
       "      <td>0.977498</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.032419</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.106614</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690225</td>\n",
       "      <td>0.689680</td>\n",
       "      <td>0.697058</td>\n",
       "      <td>0.690883</td>\n",
       "      <td>0.693118</td>\n",
       "      <td>0.689179</td>\n",
       "      <td>0.693229</td>\n",
       "      <td>0.691470</td>\n",
       "      <td>0.691820</td>\n",
       "      <td>0.002299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.034498</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.115137</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978198</td>\n",
       "      <td>0.976934</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978573</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.976584</td>\n",
       "      <td>0.976951</td>\n",
       "      <td>0.977589</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.035170</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.134524</td>\n",
       "      <td>0.023976</td>\n",
       "      <td>6</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.571661</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.542345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.674782</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>0.675990</td>\n",
       "      <td>0.673688</td>\n",
       "      <td>0.673929</td>\n",
       "      <td>0.676711</td>\n",
       "      <td>0.671869</td>\n",
       "      <td>0.674876</td>\n",
       "      <td>0.001811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.035948</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>0.119159</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>6</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.976934</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978573</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.976951</td>\n",
       "      <td>0.977644</td>\n",
       "      <td>0.000833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.033560</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.114006</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>7</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.581433</td>\n",
       "      <td>0.548860</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.539088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664244</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.662913</td>\n",
       "      <td>0.659462</td>\n",
       "      <td>0.655711</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>0.664186</td>\n",
       "      <td>0.656987</td>\n",
       "      <td>0.660674</td>\n",
       "      <td>0.002943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.032928</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.114672</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>7</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.605863</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.032866</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.117242</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>8</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.570033</td>\n",
       "      <td>0.548860</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651344</td>\n",
       "      <td>0.646984</td>\n",
       "      <td>0.650018</td>\n",
       "      <td>0.648202</td>\n",
       "      <td>0.650082</td>\n",
       "      <td>0.643428</td>\n",
       "      <td>0.652205</td>\n",
       "      <td>0.644465</td>\n",
       "      <td>0.647724</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.032896</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.119650</td>\n",
       "      <td>0.007401</td>\n",
       "      <td>8</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.607492</td>\n",
       "      <td>0.602606</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.032596</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.121994</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.576547</td>\n",
       "      <td>0.535831</td>\n",
       "      <td>0.566775</td>\n",
       "      <td>0.560261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640807</td>\n",
       "      <td>0.640988</td>\n",
       "      <td>0.641119</td>\n",
       "      <td>0.634762</td>\n",
       "      <td>0.637552</td>\n",
       "      <td>0.632716</td>\n",
       "      <td>0.640588</td>\n",
       "      <td>0.635753</td>\n",
       "      <td>0.637245</td>\n",
       "      <td>0.003919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.038507</td>\n",
       "      <td>0.008651</td>\n",
       "      <td>0.126772</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.614007</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.574919</td>\n",
       "      <td>0.535831</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633903</td>\n",
       "      <td>0.631359</td>\n",
       "      <td>0.634399</td>\n",
       "      <td>0.629495</td>\n",
       "      <td>0.626294</td>\n",
       "      <td>0.632898</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.631942</td>\n",
       "      <td>0.630126</td>\n",
       "      <td>0.003674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.032173</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.124043</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.032284</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.581433</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625908</td>\n",
       "      <td>0.622820</td>\n",
       "      <td>0.622775</td>\n",
       "      <td>0.625499</td>\n",
       "      <td>0.617396</td>\n",
       "      <td>0.624183</td>\n",
       "      <td>0.618261</td>\n",
       "      <td>0.622505</td>\n",
       "      <td>0.621772</td>\n",
       "      <td>0.003410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.035875</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>0.128507</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.033524</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.130154</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.573290</td>\n",
       "      <td>0.539088</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613372</td>\n",
       "      <td>0.616097</td>\n",
       "      <td>0.617327</td>\n",
       "      <td>0.616055</td>\n",
       "      <td>0.615399</td>\n",
       "      <td>0.615287</td>\n",
       "      <td>0.613178</td>\n",
       "      <td>0.615426</td>\n",
       "      <td>0.614489</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.033077</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.134056</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.044292</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>0.166894</td>\n",
       "      <td>0.032166</td>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.566775</td>\n",
       "      <td>0.524430</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.603561</td>\n",
       "      <td>0.613876</td>\n",
       "      <td>0.612786</td>\n",
       "      <td>0.605048</td>\n",
       "      <td>0.614016</td>\n",
       "      <td>0.608640</td>\n",
       "      <td>0.608167</td>\n",
       "      <td>0.608950</td>\n",
       "      <td>0.003561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.039248</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.169081</td>\n",
       "      <td>0.042241</td>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.038487</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.151484</td>\n",
       "      <td>0.023125</td>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.563518</td>\n",
       "      <td>0.530945</td>\n",
       "      <td>0.545603</td>\n",
       "      <td>0.530945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607740</td>\n",
       "      <td>0.603379</td>\n",
       "      <td>0.609880</td>\n",
       "      <td>0.608064</td>\n",
       "      <td>0.601598</td>\n",
       "      <td>0.605664</td>\n",
       "      <td>0.600290</td>\n",
       "      <td>0.605445</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.045463</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.175210</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.614007</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>0.599349</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.110079      0.090410         0.080513        0.016833   \n",
       "1        0.036103      0.008899         0.061289        0.004037   \n",
       "2        0.033492      0.002710         0.082002        0.004112   \n",
       "3        0.033136      0.003043         0.080544        0.007485   \n",
       "4        0.032743      0.002394         0.093691        0.009028   \n",
       "5        0.033112      0.002031         0.088803        0.004387   \n",
       "6        0.032418      0.001051         0.099414        0.005928   \n",
       "7        0.031901      0.001670         0.094736        0.003703   \n",
       "8        0.032419      0.001279         0.106614        0.006878   \n",
       "9        0.034498      0.004221         0.115137        0.014970   \n",
       "10       0.035170      0.001644         0.134524        0.023976   \n",
       "11       0.035948      0.006516         0.119159        0.024283   \n",
       "12       0.033560      0.001961         0.114006        0.006288   \n",
       "13       0.032928      0.001507         0.114672        0.005267   \n",
       "14       0.032866      0.001974         0.117242        0.007214   \n",
       "15       0.032896      0.000467         0.119650        0.007401   \n",
       "16       0.032596      0.001533         0.121994        0.003780   \n",
       "17       0.038507      0.008651         0.126772        0.009246   \n",
       "18       0.032313      0.001349         0.123737        0.004317   \n",
       "19       0.032173      0.001543         0.124043        0.003512   \n",
       "20       0.032284      0.002249         0.128205        0.004289   \n",
       "21       0.035875      0.011388         0.128507        0.008269   \n",
       "22       0.033524      0.002720         0.130154        0.002000   \n",
       "23       0.033077      0.002357         0.134056        0.007860   \n",
       "24       0.044292      0.013619         0.166894        0.032166   \n",
       "25       0.039248      0.008462         0.169081        0.042241   \n",
       "26       0.038487      0.010152         0.151484        0.023125   \n",
       "27       0.045463      0.012321         0.175210        0.026391   \n",
       "\n",
       "   param_classifier__n_neighbors param_classifier__weights  split0_test_score  \\\n",
       "0                              1                   uniform           0.587948   \n",
       "1                              1                  distance           0.587948   \n",
       "2                              2                   uniform           0.584691   \n",
       "3                              2                  distance           0.589577   \n",
       "4                              3                   uniform           0.568404   \n",
       "5                              3                  distance           0.596091   \n",
       "6                              4                   uniform           0.558632   \n",
       "7                              4                  distance           0.597720   \n",
       "8                              5                   uniform           0.561889   \n",
       "9                              5                  distance           0.597720   \n",
       "10                             6                   uniform           0.571661   \n",
       "11                             6                  distance           0.612378   \n",
       "12                             7                   uniform           0.581433   \n",
       "13                             7                  distance           0.605863   \n",
       "14                             8                   uniform           0.570033   \n",
       "15                             8                  distance           0.607492   \n",
       "16                             9                   uniform           0.576547   \n",
       "17                             9                  distance           0.614007   \n",
       "18                            10                   uniform           0.574919   \n",
       "19                            10                  distance           0.612378   \n",
       "20                            11                   uniform           0.581433   \n",
       "21                            11                  distance           0.612378   \n",
       "22                            12                   uniform           0.573290   \n",
       "23                            12                  distance           0.615635   \n",
       "24                            13                   uniform           0.566775   \n",
       "25                            13                  distance           0.615635   \n",
       "26                            14                   uniform           0.563518   \n",
       "27                            14                  distance           0.614007   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score       ...         \\\n",
       "0            0.592834           0.587948           0.591205       ...          \n",
       "1            0.592834           0.587948           0.591205       ...          \n",
       "2            0.565147           0.576547           0.592834       ...          \n",
       "3            0.597720           0.587948           0.584691       ...          \n",
       "4            0.552117           0.578176           0.561889       ...          \n",
       "5            0.597720           0.594463           0.605863       ...          \n",
       "6            0.568404           0.560261           0.565147       ...          \n",
       "7            0.600977           0.586319           0.596091       ...          \n",
       "8            0.561889           0.547231           0.547231       ...          \n",
       "9            0.600977           0.591205           0.591205       ...          \n",
       "10           0.543974           0.552117           0.542345       ...          \n",
       "11           0.596091           0.594463           0.600977       ...          \n",
       "12           0.548860           0.543974           0.539088       ...          \n",
       "13           0.597720           0.586319           0.594463       ...          \n",
       "14           0.548860           0.558632           0.550489       ...          \n",
       "15           0.602606           0.586319           0.584691       ...          \n",
       "16           0.535831           0.566775           0.560261       ...          \n",
       "17           0.591205           0.597720           0.596091       ...          \n",
       "18           0.535831           0.565147           0.550489       ...          \n",
       "19           0.594463           0.597720           0.587948       ...          \n",
       "20           0.543974           0.550489           0.558632       ...          \n",
       "21           0.589577           0.591205           0.600977       ...          \n",
       "22           0.539088           0.543974           0.547231       ...          \n",
       "23           0.592834           0.600977           0.596091       ...          \n",
       "24           0.524430           0.543974           0.543974       ...          \n",
       "25           0.586319           0.596091           0.600977       ...          \n",
       "26           0.530945           0.545603           0.530945       ...          \n",
       "27           0.584691           0.599349           0.589577       ...          \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.978016            0.976381            0.973665   \n",
       "1             0.978016            0.976381            0.973665   \n",
       "2             0.790153            0.791061            0.791137   \n",
       "3             0.977108            0.977108            0.974392   \n",
       "4             0.754360            0.756177            0.758264   \n",
       "5             0.978743            0.978379            0.976934   \n",
       "6             0.713118            0.715480            0.721032   \n",
       "7             0.978924            0.978379            0.977116   \n",
       "8             0.690225            0.689680            0.697058   \n",
       "9             0.979106            0.978198            0.976934   \n",
       "10            0.674419            0.674782            0.676171   \n",
       "11            0.979106            0.978743            0.976934   \n",
       "12            0.664244            0.662791            0.662913   \n",
       "13            0.979106            0.978743            0.977116   \n",
       "14            0.651344            0.646984            0.650018   \n",
       "15            0.979106            0.978743            0.977116   \n",
       "16            0.640807            0.640988            0.641119   \n",
       "17            0.979106            0.978743            0.977116   \n",
       "18            0.633903            0.631359            0.634399   \n",
       "19            0.979106            0.978743            0.977116   \n",
       "20            0.625908            0.622820            0.622775   \n",
       "21            0.979106            0.978743            0.977116   \n",
       "22            0.613372            0.616097            0.617327   \n",
       "23            0.979106            0.978743            0.977116   \n",
       "24            0.609375            0.603561            0.613876   \n",
       "25            0.979106            0.978743            0.977116   \n",
       "26            0.607740            0.603379            0.609880   \n",
       "27            0.979106            0.978743            0.977116   \n",
       "\n",
       "    split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0             0.975300            0.977302            0.974582   \n",
       "1             0.975300            0.977302            0.974582   \n",
       "2             0.788231            0.789541            0.789942   \n",
       "3             0.975300            0.976938            0.974946   \n",
       "4             0.751181            0.751770            0.753631   \n",
       "5             0.977661            0.978028            0.976398   \n",
       "6             0.713767            0.714545            0.715142   \n",
       "7             0.977842            0.978573            0.976761   \n",
       "8             0.690883            0.693118            0.689179   \n",
       "9             0.977842            0.978573            0.976761   \n",
       "10            0.675990            0.673688            0.673929   \n",
       "11            0.977842            0.978573            0.976761   \n",
       "12            0.659462            0.655711            0.657407   \n",
       "13            0.977842            0.978754            0.976943   \n",
       "14            0.648202            0.650082            0.643428   \n",
       "15            0.977842            0.978754            0.976943   \n",
       "16            0.634762            0.637552            0.632716   \n",
       "17            0.977842            0.978754            0.976943   \n",
       "18            0.629495            0.626294            0.632898   \n",
       "19            0.977842            0.978754            0.976943   \n",
       "20            0.625499            0.617396            0.624183   \n",
       "21            0.977842            0.978754            0.976943   \n",
       "22            0.616055            0.615399            0.615287   \n",
       "23            0.977842            0.978754            0.976943   \n",
       "24            0.612786            0.605048            0.614016   \n",
       "25            0.977842            0.978754            0.976943   \n",
       "26            0.608064            0.601598            0.605664   \n",
       "27            0.977842            0.978754            0.976943   \n",
       "\n",
       "    split8_train_score  split9_train_score  mean_train_score  std_train_score  \n",
       "0             0.975676            0.971869          0.975410         0.001669  \n",
       "1             0.975676            0.971869          0.975410         0.001669  \n",
       "2             0.792521            0.790563          0.790436         0.001153  \n",
       "3             0.974769            0.975862          0.975791         0.000936  \n",
       "4             0.755491            0.750998          0.753969         0.002185  \n",
       "5             0.976765            0.975681          0.977244         0.000899  \n",
       "6             0.718279            0.711615          0.715666         0.002834  \n",
       "7             0.976584            0.976225          0.977498         0.000859  \n",
       "8             0.693229            0.691470          0.691820         0.002299  \n",
       "9             0.976584            0.976951          0.977589         0.000796  \n",
       "10            0.676711            0.671869          0.674876         0.001811  \n",
       "11            0.976765            0.976951          0.977644         0.000833  \n",
       "12            0.664186            0.656987          0.660674         0.002943  \n",
       "13            0.976765            0.977132          0.977734         0.000804  \n",
       "14            0.652205            0.644465          0.647724         0.002933  \n",
       "15            0.976765            0.977132          0.977734         0.000804  \n",
       "16            0.640588            0.635753          0.637245         0.003919  \n",
       "17            0.976765            0.977132          0.977734         0.000804  \n",
       "18            0.627700            0.631942          0.630126         0.003674  \n",
       "19            0.976765            0.977132          0.977734         0.000804  \n",
       "20            0.618261            0.622505          0.621772         0.003410  \n",
       "21            0.976765            0.977132          0.977734         0.000804  \n",
       "22            0.613178            0.615426          0.614489         0.002754  \n",
       "23            0.976765            0.977132          0.977734         0.000804  \n",
       "24            0.608640            0.608167          0.608950         0.003561  \n",
       "25            0.976765            0.977132          0.977734         0.000804  \n",
       "26            0.600290            0.605445          0.604700         0.003175  \n",
       "27            0.976765            0.977132          0.977734         0.000804  \n",
       "\n",
       "[28 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_knn.head()\n",
    "results_knn.drop(['params'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'gini'}\n",
      "10-fold CV          : 0.5628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "dt_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "dt_grid.fit(X, y)\n",
    "\n",
    "print(dt_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, dt_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__criterion</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.214126</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.555375</td>\n",
       "      <td>0.573290</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.540850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.436205</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>0.535831</td>\n",
       "      <td>0.530945</td>\n",
       "      <td>0.576547</td>\n",
       "      <td>0.517974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.214126      0.021468         0.001572        0.000744   \n",
       "1       0.436205      0.041683         0.001169        0.000277   \n",
       "\n",
       "  param_classifier__criterion  split0_test_score  split1_test_score  \\\n",
       "0                        gini           0.589577           0.555375   \n",
       "1                     entropy           0.558632           0.535831   \n",
       "\n",
       "   split2_test_score  split3_test_score  split4_test_score       ...         \\\n",
       "0           0.573290           0.552117           0.540850       ...          \n",
       "1           0.530945           0.576547           0.517974       ...          \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.979106            0.978743            0.977116   \n",
       "1            0.979106            0.978743            0.977116   \n",
       "\n",
       "   split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0            0.977842            0.978754            0.976943   \n",
       "1            0.977842            0.978754            0.976943   \n",
       "\n",
       "   split8_train_score  split9_train_score  mean_train_score  std_train_score  \n",
       "0            0.976765            0.977132          0.977734         0.000804  \n",
       "1            0.976765            0.977132          0.977734         0.000804  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dt = pd.DataFrame(dt_grid.cv_results_)\n",
    "results_dt.head()\n",
    "results_dt.drop(['params'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    -0.051288\n",
      "1    -0.067283\n",
      "2     0.012292\n",
      "3    -0.056761\n",
      "4          NaN\n",
      "5    -0.020025\n",
      "6     0.006412\n",
      "7     0.015327\n",
      "8     0.042063\n",
      "9     0.030478\n",
      "10   -0.202562\n",
      "11   -0.188363\n",
      "12   -0.155318\n",
      "13   -0.052812\n",
      "14    0.040710\n",
      "15   -0.002808\n",
      "16   -0.012917\n",
      "17   -0.045495\n",
      "18   -0.191722\n",
      "19   -0.177150\n",
      "20   -0.069303\n",
      "21   -0.047792\n",
      "22   -0.053727\n",
      "23   -0.010610\n",
      "24   -0.032321\n",
      "25   -0.091450\n",
      "26   -0.087245\n",
      "27   -0.104309\n",
      "28   -0.057786\n",
      "29   -0.082848\n",
      "30   -0.084612\n",
      "31    0.002486\n",
      "32    0.040710\n",
      "33   -0.030349\n",
      "34         NaN\n",
      "35    0.028871\n",
      "36    0.070051\n",
      "37    0.041048\n",
      "38   -0.062701\n",
      "39   -0.085992\n",
      "40   -0.057328\n",
      "41    0.019764\n",
      "42   -0.062534\n",
      "43   -0.039263\n",
      "44   -0.033728\n",
      "45   -0.058661\n",
      "46    0.017486\n",
      "47    0.018575\n",
      "48   -0.064887\n",
      "49   -0.055400\n",
      "50   -0.046700\n",
      "51    0.048953\n",
      "52    0.031527\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corr = df.corr\n",
    "print(df.drop(\"heuristic\", axis=1).apply(lambda x: x.corr(df.heuristic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition based on reference: https://pyswarms.readthedocs.io/en/latest/examples/feature_subset_selection.html#using-binary-pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_per_particle(m, alpha):\n",
    "    if np.count_nonzero(m) == 0:\n",
    "        X_subset = X\n",
    "    else:\n",
    "        X_subset = X[m]\n",
    "    P = X_subset.apply(lambda x: x.corr(df.heuristic)).sum()\n",
    "    return (alpha * (1.0 - P)\n",
    "        + (1.0 - alpha) * (1 - (X_subset.shape[1] / len(X.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, alpha = 0.8):\n",
    "    n_particles = x.shape[0]\n",
    "    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]\n",
    "    return np.array(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyswarms as ps\n",
    "from pyswarms.discrete import BinaryPSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n",
    "dimensions = 52 # dimensions should be the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=dimensions, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.discrete.binary:Arguments Passed to Objective Function: {}\n",
      "INFO:pyswarms.discrete.binary:Iteration 1/1000, cost: 3.1932794070817994\n",
      "INFO:pyswarms.discrete.binary:Iteration 101/1000, cost: 3.0525275363493383\n",
      "INFO:pyswarms.discrete.binary:Iteration 201/1000, cost: 3.0269362871252543\n",
      "INFO:pyswarms.discrete.binary:Iteration 301/1000, cost: 3.0269362871252543\n",
      "INFO:pyswarms.discrete.binary:Iteration 401/1000, cost: 3.0269362871252543\n",
      "INFO:pyswarms.discrete.binary:Iteration 501/1000, cost: 3.0269362871252543\n",
      "INFO:pyswarms.discrete.binary:Iteration 601/1000, cost: 3.014140662513213\n",
      "INFO:pyswarms.discrete.binary:Iteration 701/1000, cost: 3.014140662513213\n",
      "INFO:pyswarms.discrete.binary:Iteration 801/1000, cost: 3.014140662513213\n",
      "INFO:pyswarms.discrete.binary:Iteration 901/1000, cost: 3.014140662513213\n",
      "INFO:pyswarms.discrete.binary:================================\n",
      "Optimization finished!\n",
      "Final cost: 2.9885\n",
      "Best value: [ 0.000000 0.000000 0.000000 ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost, pos = optimizer.optimize(f, print_step=100, iters=1000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'gini'}\n",
      "10-fold CV          : 0.5155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "dt_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "dt_grid.fit(X[pos], y)\n",
    "\n",
    "print(dt_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, dt_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 14, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.5159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:   10.7s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__n_neighbors': range(1,15),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "knn_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "knn_grid.fit(X[pos], y)\n",
    "\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 14, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.5147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    4.8s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('pca', PCA(0.96)),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__n_neighbors': range(1,15),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "knn_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "knn_grid.fit(X[pos], y)\n",
    "\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'gini'}\n",
      "10-fold CV          : 0.5126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('pca', PCA(0.96)),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "dt_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "dt_grid.fit(X[pos], y)\n",
    "\n",
    "print(dt_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, dt_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    1.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 13, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    6.6s finished\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('lda', LinearDiscriminantAnalysis()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__n_neighbors': range(1,15),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "knn_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "knn_grid.fit(X[pos], y)\n",
    "\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'gini'}\n",
      "10-fold CV          : 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('lda', LinearDiscriminantAnalysis()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "dt_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "dt_grid.fit(X[pos], y)\n",
    "\n",
    "print(dt_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, dt_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
