{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados e problema\n",
    "Consideramos nesse trabalho o dataset criado e usado no artigo [\"Machine Learning for First-Order Theorem Proving\" (Bridge, Holden, & Paulson, 2014)](https://link.springer.com/article/10.1007/s10817-014-9301-5), obtido através do [repositório de aprendizado de máquina da UCI](http://archive.ics.uci.edu/ml/datasets/First-order+theorem+proving).\n",
    "\n",
    "Nesse artigo os autores apresentam a dificuldade, no uso de um provador automático de teoremas de primeira ordem, de selecionar a melhor heurística para provar o teorema em questão. A melhor heurística a ser usada vai depender das formas da conjectura a ser provada e dos axiomas dados. Porém, o relacionamento entre essas formas e a melhor heurítica a ser usada não é obvia, mesmo para aqueles com experiência extensiva com uso de provadores automáticos.\n",
    "\n",
    "Portando, os autores propõe determinar a melhor heurística a ser usada a partir de certas características extraídas da conjectura e dos axiomas; e fazê-lo de forma automática, com o uso de aprendizado de máquina.\n",
    "    \n",
    "Para isso, foram escolhidos 6118 problemas de demonstrações de primeira ordem da biblioteca TPTP (Thousands of Problems for Theorem Provers), descrita em [\"The TPTP problem library and associated infrastructure\" (Sutcliffe, 2009)](https://link.springer.com/article/10.1007/s10817-009-9143-8), e foi usado [E (versão 0.99 Singtom)](https://wwwlehre.dhbw-stuttgart.de/~sschulz/E/E.html), um provador automático de alta performance para lógica de primeira ordem completa com igualdade. Das 82 heurísticas incluídas nesse provador, foram escolhidas as cinco mais frequentemente selecionados pelo E para os problemas da TPTP, diminuindo o esforço de construção e uso do dataset.\n",
    "\n",
    "O objetivo então é selecionar para cada um dos problemas qual das cinco heuríticas em consideração (H1, ..., H5) é a melhor, onde uma heurística é melhor que outra se leva a uma demonstração mais rapidamente em E. Uma \"heurística\" adicional (H0) é considerada, a ser selecionada quando nenhuma das outras heurísticas produz uma demonstração dentro do tempo limite de 100 segundos de CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos\n",
    "Os atributos escolhidas pelos autores para realizar essa tarefa de aprendizado podem ser divididas em dois tipos: atributos estáticos e atributos dinâmicos. Enquanto atributos estáticos são características que podem ser extraídas somente da descrição do problema (sua conjectura e seus axiomas), atributos dinâmicos são obtidos a partir de características de cláusulas geradas nos estágios iniciais de uma demonstração automática.\n",
    "Descrevemos abaixo alguns dos 14 atributos estáticos, para exemplificação. Os 39 atributos dinâmicos seguem formas similares, e detalhes podem ser obtidos de Bridge et al., 2014.\n",
    "\n",
    "1\\. Fração de cláusulas que são _unit clauses_ (consiste de exatamente um literal)<br>\n",
    "2\\. Fração de cláusulas que são _Horn clauses_ (contêm no máximo um literal não-negado)<br>\n",
    "7\\. Fração de cláusulas que são puramente negativas<br>\n",
    "9\\. Comprimento de clásula máximo<br>\n",
    "10\\. Comprimento de clásula médio<br>\n",
    "11\\. Profundidade de cláusula máxima.<br>\n",
    "13\\. Peso de cláusula máximo.<br>\n",
    "\n",
    "Todos os 53 atributos são numéricos, sendo os atributos 9, 11, e 13 discretos e o restante contínuo. Não há valores faltosos para nenhum dos atributos, em nenhuma das amostras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramentas utilizadas\n",
    "Usaremos em todas as estapas desse trabalho a linguagem Python, pelas diversas bibliotecas disponíveis nela que facilitam manipulação de dados, tarefas de aprendizado de máquina, e afins. Mais especificamente, usaremos as bibliotecas Pandas e Numpy, que contêm ferramentas de análise de dados, e provêm estruturas de dados e operações para manipulá-las úteis para trabalhar com dados multi-dimensionais.\n",
    "\n",
    "Quanto aos algoritmos, usaremos principalmente a biblioteca de aprendizado de máquina scikit-learn, que oferece algoritmos de aprendizado como K-nearest neighbors, e árvores de decisão; métodos de redução de dimensionalidade como _principal component analysis_ (PCA), e _linear discriminant analysis_ (LDA); e funções de validação e métricas de classificação. Além disso, usamos as bibliotecas pyswarms e pyeasyga pelas suas implementações de _particle swarm optimization_ (PSO) e de algoritmo genético, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestão e pre-processamento de dados\n",
    "O dataset inicialmente contém, além dos 53 atributos, 5 colunas indicando o tempo que cada heurística levou para chegar a uma demonstração para cada problema, ou -100 caso o tempo limite tenha sido atingido antes disso. Verificamos primeiramente as dimensões dos dados: 6118 instâncias e 58 colunas (53 atributos e 5 heurísticas), e então visualizamos as primeiras instâncias no conjunto de dados. Note que colunas de atributos estão numeradas de 0 a 52, e as colunas com os tempos de cada heurística de 53 a 57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/all-data-raw.csv\", header=None)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como apenas nos interessa para essa tarefa qual foi a melhor heurística para cada um, transformamos os dataset para refletir isso, após importá-lo, adicionando uma coluna \"heuristic\" com valores de 0 a 5 refletindo quais das heurísticas é a melhor para cada amostra (1 a 5) ou se nenhuma é suficiente (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_heuristic(row, time_cols):\n",
    "    n_heuristics = 5\n",
    "    h_times = row[time_cols].reset_index(drop=True)\n",
    "    h_times.replace({-100.0 : np.nan}, inplace=True)\n",
    "    idx, min_time = h_times.idxmin(), h_times.min()\n",
    "    if np.isnan(min_time):\n",
    "       return 0\n",
    "    else:\n",
    "       return idx+1\n",
    "\n",
    "time_cols = list(range(53, 58))\n",
    "df['heuristic'] = df.apply(lambda r : best_heuristic(r, time_cols), axis=1)\n",
    "df.drop(time_cols, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de agora dividimos os dados entre X e y, representando respectivamente a matriz de atributos e o vetor de _labels_ (nesse caso, a melhor heurística)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(['heuristic'], axis=1).astype('float64'), df['heuristic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar então, abaixo, na frequência com que cada heurística foi a melhor para uma certa instância, um certo desbalanceamento de classes. Enquanto a classe 0 é a mais frequente, com 2554 instâncias, classe 2 é a menos, com 486."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, podemos ter uma noção também da distribuição de cada atributo. Note que os atributos 5 e 35 (aqui numerados 4 e 34) são constantes: no artigo original os autores optaram por eliminar esses dois atributos, mas para os propósitos desse trabalho, deixamos todos e observaremos se cada um dos métodos de seleção de atributos elimina ou mantêm esses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().transpose()[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar também o valor absoluto da correlação de Pearson entre cada atributo e a _label_, que será mais tarde usada para seleção de atributos com o algoritmo genético e o de PSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "corr = abs(X.corrwith(y))\n",
    "\n",
    "palette = np.array(sns.color_palette(\"Blues_d\", len(corr)))\n",
    "rank = np.array(corr.rank().fillna(0), dtype=int)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.barplot(x=corr.index, y=corr, palette=palette[rank])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de validação e normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "X, X_val, y, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X.update(scaler.fit_transform(X))\n",
    "X_val.update(scaler.transform(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa sessão utilizamos os dois classificadores que serão utilizados para os métodos de extratificação e filtragem dos _features_. O _K-Nearest Neighbors_ (KNN) e a _Decision Tree_ (DT), com as implementações fornecida pelo pacote scikit-learn. Mais informações sobre a implementação podem ser acessados nos links:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart\n",
    "\n",
    "Utilizamos o _Stratified K-fold_ que é uma metodologia de validação cruzada com k divisões conjuntos de treino e teste, para selecionarmos os melhores parâmetros dos algoritmos para os dados e então testamos com o conjunto de validação separado previamente. \n",
    "\n",
    "Utilizamos a acurácia como métrica de avaliação dos modelos gerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def base_classifiers(X, y, X_val, y_val):\n",
    "    classifier_params = {\n",
    "        KNeighborsClassifier() : {\n",
    "            'n_neighbors': list(range(1,16)),\n",
    "            'weights' : ['distance', 'uniform']\n",
    "        },\n",
    "        DecisionTreeClassifier() : {\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_grid_results = {}\n",
    "    for classifier, params in classifier_params.items():\n",
    "\n",
    "        kfold = StratifiedKFold(10, shuffle=True, random_state=random_seed)\n",
    "        cv_grid = GridSearchCV(classifier, params, scoring='accuracy', \n",
    "                               cv=kfold, verbose=1, n_jobs=-1)\n",
    "        cv_grid.fit(X, y)\n",
    "\n",
    "        class_ = type(classifier).__name__\n",
    "        print(class_, cv_grid.best_params_)\n",
    "        print(\"10-fold CV mean score: {:.4f}\".format(cv_grid.best_score_))\n",
    "        cv_grid_results[class_] = pd.DataFrame(cv_grid.cv_results_)\n",
    "        \n",
    "        best = cv_grid.best_estimator_\n",
    "        y_pred = best.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(\"final validation score:\", accuracy)\n",
    "        print()\n",
    "        \n",
    "base_classifiers(X, y, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils.text import columnize\n",
    "\n",
    "abs_correlations = abs(X.corrwith(y))\n",
    "ord_corr = abs_correlations.sort_values(ascending=False)\n",
    "corr_pretty_list = list(map(lambda x : \"{:2} {:.4f}\".format(x[0], x[1]), zip(ord_corr.index, ord_corr)))\n",
    "print(columnize(corr_pretty_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic\n",
    "Utilizando a biblioteca pyeasyga, fizemos uso do algoritmo evolucionário genético para selecionar os atributos com base na função de correlação dos atributos com os _targets_, tentando maximizar a correlação média dos atributos como sugerido em sala de aula. Para cara configuração foram executados 10 vezes. Também executamos uma vez utilizando a acurácia de um modelo _KNN_ com  k = 5 e distancia simples,  dentro de cada individuo, desse modo queremos maximizar a acurácia do modelo e depois testamos com os demais parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyeasyga import pyeasyga\n",
    "from itertools import compress\n",
    "\n",
    "def fitness_corr(individual, idx_corr):\n",
    "    fitness = 0\n",
    "    n = individual.count(1)\n",
    "    if n > 0:\n",
    "        fitness = sum(corr for idx, corr in compress(idx_corr, individual))\n",
    "        fitness /= n\n",
    "    return fitness\n",
    "\n",
    "def fitness_knn(individual, X_y):\n",
    "    X, y = X_y\n",
    "    fitness = 0\n",
    "    selected_attrs = list(compress(X.columns, individual))\n",
    "    if selected_attrs:\n",
    "        X = X[selected_attrs]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, stratify=y, test_size=0.3, random_state=random_seed)\n",
    "\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        fitness = accuracy_score(y_test, y_pred)\n",
    "    return fitness\n",
    "\n",
    "def genetic(data, fitness_function, generations, rep):\n",
    "    for _ in range(rep):\n",
    "        ga = pyeasyga.GeneticAlgorithm(data, generations=generations)\n",
    "\n",
    "        ga.fitness_function = fitness_function\n",
    "        ga.run()\n",
    "\n",
    "        score, individual = ga.best_individual()\n",
    "        selected_attrs = list(compress(X.columns, individual))\n",
    "        print(score, len(selected_attrs), selected_attrs)\n",
    "        print()\n",
    "\n",
    "        base_classifiers(X[selected_attrs], y, X_val[selected_attrs], y_val)\n",
    "\n",
    "print(\"correlation\")\n",
    "abs_correlations = abs(X.corrwith(y)).fillna(0)\n",
    "idx_corr = list(zip(abs_correlations.index, abs_correlations))\n",
    "genetic(idx_corr, fitness_corr, 50, 10)\n",
    "\n",
    "print(\"knn\")\n",
    "genetic((X, y), fitness_knn, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO\n",
    "\n",
    "Function definition based on reference: https://pyswarms.readthedocs.io/en/latest/examples/feature_subset_selection.html#using-binary-pso\n",
    "Utilizando a memsma abordagem do algoritmo genético, utilizamos o PSO da biblioteca pyswarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyswarms as ps\n",
    "\n",
    "def f_per_particle_corr(m, alpha):\n",
    "    if np.count_nonzero(m) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        X_subset = X.loc[:, m==1]\n",
    "    P = abs(X_subset.corrwith(y)).sum()\n",
    "    P /= np.count_nonzero(m) \n",
    "    return (alpha * (1.0 - P))\n",
    "\n",
    "def f_corr(x, alpha = 1.0):\n",
    "    n_particles = x.shape[0]\n",
    "    j = [f_per_particle_corr(x[i], alpha) for i in range(n_particles)]\n",
    "    return np.array(j)\n",
    "\n",
    "def f_per_particle_knn(m, alpha = 1.0):\n",
    "    if np.count_nonzero(m) == 0:\n",
    "        X_subset = X\n",
    "    else:\n",
    "        X_subset = X.loc[:, m==1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_subset, y, stratify=y, test_size=0.3)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return alpha * (1 - accuracy_score(y_test, y_pred))\n",
    "\n",
    "def f_knn(x, alpha = 1.0):\n",
    "    n_particles = x.shape[0]\n",
    "    j = [f_per_particle_knn(x[i], alpha) for i in range(n_particles)]\n",
    "    return np.array(j)\n",
    "\n",
    "options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n",
    "dimensions = 53 # dimensions should be the number of features\n",
    "\n",
    "def pso(cost_function, iters, rep):\n",
    "    for _ in range(rep):\n",
    "        optimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=dimensions, options=options)\n",
    "\n",
    "        cost, pos = optimizer.optimize(cost_function, print_step=100, iters=iters, verbose=2)\n",
    "\n",
    "        selected_attrs = list(compress(X.columns, pos))\n",
    "        print(cost, len(selected_attrs), selected_attrs)\n",
    "        print()\n",
    "\n",
    "        base_classifiers(X[selected_attrs], y, X_val[selected_attrs], y_val)\n",
    "\n",
    "print(\"correlation\")\n",
    "pso(f_corr, 1000, 10)\n",
    "print(\"knn\")\n",
    "pso(f_knn, 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Tendo cada instancia em um espaço m-dimensional,vamos então mapear através de uma transformação linear as imagens do espaço orginalmente m-dimensional em um outro espaço n-dimensional, tal que  n < m. Onde calculamos a média:\n",
    "    $$ \\mu = \\frac{1}{k}\\sum\\limits_{i=1}^kx_i$$\n",
    "       e a matriz de covariância:\n",
    "    $$ S = \\frac{1}{k}\\sum\\limits_{i=1}^k(x_i - \\mu)(x_i - \\mu)^T $$\n",
    "    \n",
    "   Para então computar os autovalores $ \\lambda_i $ e autovetores $v_i$ da matriz de covariância S:\n",
    "    $$S v_i = \\lambda_iv_i, i = 1,2, \\ldots, k $$\n",
    "    Com isso, podemos ter uma noção da quantidade de informação baseada no quanto cada autovalor representa de informação, divindo cada autovalor pela soma de todos. Ordenando de forma não-crescente, podemos retirar $Z$ componentes, reduzindo a dimensão. Sendo assim, utilizando a matriz $W$ formada pelos $K-Z$ autovetores, podemos aplicar a transformação  na matriz que representa o conjunto de dados, gerando o novo conjunto de descritores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.96)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "base_classifiers(pca.transform(X), y, pca.transform(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "Para realizar o LDA, calculamos duas matrizes  $ S_B $ e $ S_W $, respectivamente a matriz de dispersão entre classes e a matriz de dispersão dentro das classes:\n",
    "    $$S_B = \\sum_{i=1}^cM_i(\\mu_i - \\mu)(\\mu_i - \\mu)^T$$\n",
    "    $$S_W = \\sum_{i=1}^c\\sum_{x \\in C_i}(x - \\mu_i)(x - \\mu_i)^T$$\n",
    "onde $X = {x_1, x_2, \\ldots, x_n}$ e $C = C_1, C_2, \\ldots, C_c$, representando as instancias x e as classses C's e:\n",
    "$$\\mu_i = \\frac{1}{M_i}\\sum_{x \\in C_i}x\\]\\[\\mu =\\frac{1}{M}\\sum_{k=1}^M x_k$$\n",
    "\n",
    "\tSe $S_W$ for não singular, a matriz $W_{opt}$ é o operador de projeção linear ótimo, maximizando  a razão entre os determinantes das matrizes \\(S_B \\) e $S_W$\n",
    "    $$W_{opt} = arg max_w\\frac{|W^TS_BW|}{|W^TS_WW|}$$\n",
    "    \n",
    "    Espefificando a quantidade de autovetores para \\(|C|\\), ou seja, a quantidade de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X,y)\n",
    "base_classifiers(lda.transform(X), y, lda.transform(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifiers(X.iloc[:, :14], y, X_val.iloc[:, :14], y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
