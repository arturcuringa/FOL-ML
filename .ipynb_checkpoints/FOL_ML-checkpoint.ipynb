{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features:\n",
    "1. Fraction of clauses that are unit clauses. <!-- exactly one literal -->\n",
    "2. Fraction of clauses that are Horn clauses. <!-- at most one non-negated literal -->\n",
    "3. Fraction of clauses that are ground Clauses. <!-- ? -->\n",
    "4. Fraction of clauses that are demodulators. <!-- equality used as rule to rewrite newly inferred clause -->\n",
    "5. Fraction of clauses that are rewrite rules (oriented demodulators). <!-- ? -->\n",
    "6. Fraction of clauses that are purely positive.\n",
    "7. Fraction of clauses that are purely negative.\n",
    "8. Fraction of clauses that are mixed positive and negative.\n",
    "9. Maximum clause length. <!-- number of literals -->\n",
    "10. Average clause length.\n",
    "11. Maximum clause depth. <!-- see below -->\n",
    "12. Average clause depth.\n",
    "13. Maximum clause weight. <!-- defined by prover; probably its symbol count, excluding commas, parentheses, negation symbols, and disjunction symbols -->\n",
    "14. Average clause weight.\n",
    "\n",
    "<!-- \n",
    "Depth of Term, Atom, Literal, Clause\n",
    "* depth of variable, constant, or propositional atom: 0;\n",
    "* depth of term or atom with arguments: one more than the maximum argument depth;\n",
    "* depth of literal: depth of its atom (negation signs don't count);\n",
    "* depth of clause: maximum of depths of literals;\n",
    "* For example, p(x) | -p(f(x)) has depth 2.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73684</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73872</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77107</td>\n",
       "      <td>0.068363</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74436</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74060</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72932</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.080827</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73308</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>-100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3   4        5         6        7   8   \\\n",
       "0  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "1  0.83307  0.99682  0.83307  0.76948   0  0.77107  0.068363  0.16057   6   \n",
       "2  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "3  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "4  0.83307  0.99682  0.83307  0.76789   0  0.76948  0.069952  0.16057   6   \n",
       "\n",
       "       9    ...         48       49       50        51       52      53  \\\n",
       "0  1.2734   ...    0.73684  0.00188  0.73872  0.073308  0.18797 -100.00   \n",
       "1  1.2734   ...    0.74248  0.00188  0.74436  0.067669  0.18797    0.08   \n",
       "2  1.2734   ...    0.74060  0.00188  0.74248  0.069549  0.18797 -100.00   \n",
       "3  1.2734   ...    0.72932  0.00188  0.73120  0.080827  0.18797 -100.00   \n",
       "4  1.2734   ...    0.73120  0.00188  0.73308  0.078947  0.18797 -100.00   \n",
       "\n",
       "       54     55      56      57  \n",
       "0 -100.00 -100.0 -100.00 -100.00  \n",
       "1    0.08    0.2    0.08    0.08  \n",
       "2 -100.00 -100.0 -100.00 -100.00  \n",
       "3 -100.00 -100.0 -100.00 -100.00  \n",
       "4 -100.00 -100.0 -100.00 -100.00  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/all-data-raw.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       53      54     55      56      57\n",
      "0 -100.00 -100.00 -100.0 -100.00 -100.00\n",
      "1    0.08    0.08    0.2    0.08    0.08\n",
      "2 -100.00 -100.00 -100.0 -100.00 -100.00\n",
      "3 -100.00 -100.00 -100.0 -100.00 -100.00\n",
      "4 -100.00 -100.00 -100.0 -100.00 -100.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>heuristic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.73684</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73872</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77107</td>\n",
       "      <td>0.068363</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74436</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.74060</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.74248</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.72932</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.080827</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.99682</td>\n",
       "      <td>0.83307</td>\n",
       "      <td>0.76789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76948</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.16057</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.80639</td>\n",
       "      <td>0.99624</td>\n",
       "      <td>0.80263</td>\n",
       "      <td>0.73120</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.73308</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.18797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2        3  4        5         6        7  8  \\\n",
       "0  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "1  0.83307  0.99682  0.83307  0.76948  0  0.77107  0.068363  0.16057  6   \n",
       "2  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "3  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "4  0.83307  0.99682  0.83307  0.76789  0  0.76948  0.069952  0.16057  6   \n",
       "\n",
       "        9    ...            44       45       46       47       48       49  \\\n",
       "0  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.73684  0.00188   \n",
       "1  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.74248  0.00188   \n",
       "2  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.74060  0.00188   \n",
       "3  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.72932  0.00188   \n",
       "4  1.2734    ...      0.020202  0.80639  0.99624  0.80263  0.73120  0.00188   \n",
       "\n",
       "        50        51       52  heuristic  \n",
       "0  0.73872  0.073308  0.18797          0  \n",
       "1  0.74436  0.067669  0.18797          1  \n",
       "2  0.74248  0.069549  0.18797          0  \n",
       "3  0.73120  0.080827  0.18797          0  \n",
       "4  0.73308  0.078947  0.18797          0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_times = df.iloc[:, -5:]\n",
    "print(h_times.head())\n",
    "\n",
    "import numpy as np\n",
    "def best_heuristic(row):\n",
    "    n_heuristics = 5\n",
    "    h_times = row[-n_heuristics:].reset_index(drop=True)\n",
    "    h_times.replace({-100.0 : np.nan}, inplace=True)\n",
    "    idx, min_time = h_times.idxmin(), h_times.min()\n",
    "    if np.isnan(min_time):\n",
    "       return 0\n",
    "    else:\n",
    "       return idx+1\n",
    "\n",
    "df['heuristic'] = df.apply(best_heuristic, axis=1)\n",
    "df.drop([53, 54, 55, 56, 57], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2554\n",
       "1    1089\n",
       "3     748\n",
       "5     624\n",
       "4     617\n",
       "2     486\n",
       "Name: heuristic, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['heuristic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   54.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 12, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X, y = df.drop(['heuristic'], axis=1).astype('float64'), df['heuristic']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=44)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__n_neighbors': range(1,15),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "knn_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "knn_grid.fit(X, y)\n",
    "\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 12, 'classifier__weights': 'distance'}\n",
      "10-fold CV          : 0.6036\n"
     ]
    }
   ],
   "source": [
    "print(knn_grid.best_params_)\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, knn_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.11825981, 0.03362541, 0.03699412, 0.03575296, 0.04209585,\n",
       "        0.04558358, 0.0352242 , 0.03949215, 0.04093268, 0.05376897,\n",
       "        0.03690968, 0.03709061, 0.03668926, 0.03348157, 0.03599117,\n",
       "        0.03630178, 0.04776905, 0.04026332, 0.05947323, 0.04398112,\n",
       "        0.03845041, 0.04905937, 0.0332494 , 0.03382137, 0.03902576,\n",
       "        0.03489501, 0.03498204, 0.03381867]),\n",
       " 'std_fit_time': array([0.10088705, 0.00231472, 0.00705138, 0.00958448, 0.01108501,\n",
       "        0.01312359, 0.00456915, 0.01366133, 0.01052374, 0.01821758,\n",
       "        0.00455387, 0.00553366, 0.00529744, 0.00259006, 0.00525909,\n",
       "        0.00792402, 0.01527929, 0.00787149, 0.01822189, 0.01698977,\n",
       "        0.01046339, 0.01867844, 0.00146975, 0.00196738, 0.00593294,\n",
       "        0.00289371, 0.00512175, 0.00163039]),\n",
       " 'mean_score_time': array([0.07264209, 0.06371558, 0.08572564, 0.08180783, 0.12632093,\n",
       "        0.10404484, 0.10190511, 0.10317721, 0.13589313, 0.15261486,\n",
       "        0.11658344, 0.12129705, 0.12559822, 0.11203597, 0.12677131,\n",
       "        0.11792557, 0.15085177, 0.14758325, 0.2183883 , 0.13792605,\n",
       "        0.14222841, 0.15446668, 0.13523653, 0.13793733, 0.15359771,\n",
       "        0.14083917, 0.14009356, 0.14890647]),\n",
       " 'std_score_time': array([0.01330206, 0.00585663, 0.01090261, 0.01130135, 0.03323165,\n",
       "        0.01844918, 0.01214623, 0.02146245, 0.03243736, 0.04845683,\n",
       "        0.01854016, 0.02146056, 0.01956255, 0.00409485, 0.01451105,\n",
       "        0.00540348, 0.03495533, 0.03317197, 0.04518327, 0.02992482,\n",
       "        0.01192782, 0.03657432, 0.007196  , 0.01291022, 0.01765996,\n",
       "        0.00787867, 0.01341306, 0.02510134]),\n",
       " 'param_classifier__n_neighbors': masked_array(data=[1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9,\n",
       "                    10, 10, 11, 11, 12, 12, 13, 13, 14, 14],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__n_neighbors': 1, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 1, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 2, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 2, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 3, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 4, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 4, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 5, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 5, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 6, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 6, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 7, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 8, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 8, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 9, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 9, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 10, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 10, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 11, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 11, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 12, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 12, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 13, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 13, 'classifier__weights': 'distance'},\n",
       "  {'classifier__n_neighbors': 14, 'classifier__weights': 'uniform'},\n",
       "  {'classifier__n_neighbors': 14, 'classifier__weights': 'distance'}],\n",
       " 'split0_test_score': array([0.58794788, 0.58794788, 0.58469055, 0.58957655, 0.56840391,\n",
       "        0.59609121, 0.55863192, 0.59771987, 0.56188925, 0.59771987,\n",
       "        0.57166124, 0.61237785, 0.58143322, 0.60586319, 0.57003257,\n",
       "        0.60749186, 0.57654723, 0.61400651, 0.57491857, 0.61237785,\n",
       "        0.58143322, 0.61237785, 0.5732899 , 0.61563518, 0.56677524,\n",
       "        0.61563518, 0.56351792, 0.61400651]),\n",
       " 'split1_test_score': array([0.59283388, 0.59283388, 0.56514658, 0.59771987, 0.55211726,\n",
       "        0.59771987, 0.56840391, 0.6009772 , 0.56188925, 0.6009772 ,\n",
       "        0.54397394, 0.59609121, 0.54885993, 0.59771987, 0.54885993,\n",
       "        0.60260586, 0.53583062, 0.59120521, 0.53583062, 0.59446254,\n",
       "        0.54397394, 0.58957655, 0.53908795, 0.59283388, 0.52442997,\n",
       "        0.58631922, 0.53094463, 0.58469055]),\n",
       " 'split2_test_score': array([0.58794788, 0.58794788, 0.57654723, 0.58794788, 0.5781759 ,\n",
       "        0.59446254, 0.56026059, 0.58631922, 0.54723127, 0.59120521,\n",
       "        0.55211726, 0.59446254, 0.54397394, 0.58631922, 0.55863192,\n",
       "        0.58631922, 0.56677524, 0.59771987, 0.56514658, 0.59771987,\n",
       "        0.5504886 , 0.59120521, 0.54397394, 0.6009772 , 0.54397394,\n",
       "        0.59609121, 0.54560261, 0.59934853]),\n",
       " 'split3_test_score': array([0.59120521, 0.59120521, 0.59283388, 0.58469055, 0.56188925,\n",
       "        0.60586319, 0.56514658, 0.59609121, 0.54723127, 0.59120521,\n",
       "        0.54234528, 0.6009772 , 0.53908795, 0.59446254, 0.5504886 ,\n",
       "        0.58469055, 0.56026059, 0.59609121, 0.5504886 , 0.58794788,\n",
       "        0.55863192, 0.6009772 , 0.54723127, 0.59609121, 0.54397394,\n",
       "        0.6009772 , 0.53094463, 0.58957655]),\n",
       " 'split4_test_score': array([0.55228758, 0.55228758, 0.54248366, 0.55555556, 0.54084967,\n",
       "        0.55882353, 0.53921569, 0.55882353, 0.55392157, 0.56372549,\n",
       "        0.55392157, 0.57189542, 0.54575163, 0.57026144, 0.5620915 ,\n",
       "        0.56862745, 0.55228758, 0.57026144, 0.54248366, 0.56535948,\n",
       "        0.54575163, 0.56372549, 0.54575163, 0.57026144, 0.54901961,\n",
       "        0.57189542, 0.55228758, 0.57189542]),\n",
       " 'split5_test_score': array([0.61111111, 0.61111111, 0.60130719, 0.60784314, 0.57679739,\n",
       "        0.60784314, 0.56045752, 0.59803922, 0.58660131, 0.60947712,\n",
       "        0.57352941, 0.61928105, 0.56699346, 0.61764706, 0.56862745,\n",
       "        0.61437908, 0.55555556, 0.61601307, 0.55392157, 0.60784314,\n",
       "        0.54575163, 0.61111111, 0.54738562, 0.60457516, 0.54411765,\n",
       "        0.60457516, 0.53431373, 0.60620915]),\n",
       " 'split6_test_score': array([0.58265139, 0.58265139, 0.58592471, 0.58265139, 0.58919804,\n",
       "        0.60392799, 0.57283142, 0.60883797, 0.57283142, 0.59574468,\n",
       "        0.57446809, 0.59574468, 0.58756137, 0.60065466, 0.59247136,\n",
       "        0.599018  , 0.58265139, 0.60392799, 0.5695581 , 0.60392799,\n",
       "        0.58101473, 0.60883797, 0.57610475, 0.6202946 , 0.56628478,\n",
       "        0.61374795, 0.5695581 , 0.61538462]),\n",
       " 'split7_test_score': array([0.58852459, 0.58852459, 0.59344262, 0.58688525, 0.58196721,\n",
       "        0.61803279, 0.59016393, 0.60983607, 0.58852459, 0.60819672,\n",
       "        0.58360656, 0.6147541 , 0.56721311, 0.61803279, 0.56885246,\n",
       "        0.60983607, 0.56557377, 0.6147541 , 0.5704918 , 0.61803279,\n",
       "        0.57213115, 0.62622951, 0.5557377 , 0.61803279, 0.5557377 ,\n",
       "        0.62459016, 0.55409836, 0.62459016]),\n",
       " 'split8_test_score': array([0.58784893, 0.58784893, 0.55829228, 0.58949097, 0.55829228,\n",
       "        0.58784893, 0.58128079, 0.59934319, 0.55829228, 0.59277504,\n",
       "        0.55993432, 0.591133  , 0.57142857, 0.59934319, 0.56978654,\n",
       "        0.60262726, 0.57307061, 0.60591133, 0.55172414, 0.60426929,\n",
       "        0.54844007, 0.59605911, 0.55665025, 0.60262726, 0.56321839,\n",
       "        0.60098522, 0.56157635, 0.59605911]),\n",
       " 'split9_test_score': array([0.60690789, 0.60690789, 0.58388158, 0.62335526, 0.58388158,\n",
       "        0.62828947, 0.5625    , 0.60526316, 0.56743421, 0.61677632,\n",
       "        0.55427632, 0.62006579, 0.55592105, 0.61842105, 0.56085526,\n",
       "        0.61677632, 0.53453947, 0.62171053, 0.54769737, 0.62335526,\n",
       "        0.55427632, 0.62006579, 0.55427632, 0.61513158, 0.54605263,\n",
       "        0.60855263, 0.54769737, 0.61513158]),\n",
       " 'mean_test_score': array([0.58891795, 0.58891795, 0.57845701, 0.59055247, 0.56914024,\n",
       "        0.59986924, 0.5658712 , 0.59610984, 0.56456358, 0.59676365,\n",
       "        0.56096764, 0.60166721, 0.56080418, 0.60084995, 0.56505394,\n",
       "        0.59921543, 0.56031383, 0.60313828, 0.55622753, 0.60150376,\n",
       "        0.55818895, 0.60199412, 0.5539392 , 0.60362864, 0.55034325,\n",
       "        0.60232102, 0.54903563, 0.60166721]),\n",
       " 'std_test_score': array([0.01489828, 0.01489828, 0.01719973, 0.0166261 , 0.01477928,\n",
       "        0.01767817, 0.01309833, 0.01400123, 0.01380033, 0.01370961,\n",
       "        0.01333172, 0.0143128 , 0.01576334, 0.01452448, 0.0116998 ,\n",
       "        0.01433676, 0.01530728, 0.01439933, 0.01244246, 0.01573107,\n",
       "        0.01392185, 0.0170908 , 0.01163321, 0.01434252, 0.0123813 ,\n",
       "        0.01439436, 0.01308103, 0.0155716 ]),\n",
       " 'rank_test_score': array([14, 14, 16, 13, 17,  9, 18, 12, 20, 11, 21,  5, 22,  8, 19, 10, 23,\n",
       "         2, 25,  7, 24,  4, 26,  1, 27,  3, 28,  5], dtype=int32),\n",
       " 'split0_train_score': array([0.97547238, 0.97547238, 0.7916061 , 0.97565407, 0.75345203,\n",
       "        0.97710756, 0.71420785, 0.97747093, 0.68986192, 0.97765262,\n",
       "        0.67296512, 0.97747093, 0.66188227, 0.97765262, 0.64534884,\n",
       "        0.97765262, 0.62899709, 0.97765262, 0.62191134, 0.97765262,\n",
       "        0.61518895, 0.97765262, 0.60701308, 0.97765262, 0.60483285,\n",
       "        0.97765262, 0.60010901, 0.97765262]),\n",
       " 'split1_train_score': array([0.97583576, 0.97583576, 0.78960756, 0.97583576, 0.75436047,\n",
       "        0.97674419, 0.71947674, 0.97710756, 0.69349564, 0.97728924,\n",
       "        0.67823401, 0.97728924, 0.66115552, 0.97728924, 0.64516715,\n",
       "        0.97728924, 0.63917151, 0.97728924, 0.63135901, 0.97728924,\n",
       "        0.62318314, 0.97728924, 0.61573401, 0.97728924, 0.60919331,\n",
       "        0.97728924, 0.60483285, 0.97728924]),\n",
       " 'split2_train_score': array([0.97801599, 0.97801599, 0.79015262, 0.97710756, 0.75436047,\n",
       "        0.97874273, 0.71311773, 0.97892442, 0.69022529, 0.9791061 ,\n",
       "        0.6744186 , 0.9791061 , 0.66424419, 0.9791061 , 0.65134448,\n",
       "        0.9791061 , 0.64080669, 0.9791061 , 0.63390262, 0.9791061 ,\n",
       "        0.62590843, 0.9791061 , 0.61337209, 0.9791061 , 0.609375  ,\n",
       "        0.9791061 , 0.60773983, 0.9791061 ]),\n",
       " 'split3_train_score': array([0.97638081, 0.97638081, 0.79106105, 0.97710756, 0.75617733,\n",
       "        0.97837936, 0.71547965, 0.97837936, 0.68968023, 0.97819767,\n",
       "        0.67478198, 0.97874273, 0.6627907 , 0.97874273, 0.64698401,\n",
       "        0.97874273, 0.64098837, 0.97874273, 0.63135901, 0.97874273,\n",
       "        0.62281977, 0.97874273, 0.61609738, 0.97874273, 0.60356105,\n",
       "        0.97874273, 0.60337936, 0.97874273]),\n",
       " 'split4_train_score': array([0.97366509, 0.97366509, 0.79113694, 0.97439157, 0.75826371,\n",
       "        0.97693425, 0.7210316 , 0.97711587, 0.69705776, 0.97693425,\n",
       "        0.67617145, 0.97693425, 0.66291319, 0.97711587, 0.65001816,\n",
       "        0.97711587, 0.64111878, 0.97711587, 0.63439884, 0.97711587,\n",
       "        0.62277515, 0.97711587, 0.61732655, 0.97711587, 0.61387577,\n",
       "        0.97711587, 0.60988013, 0.97711587]),\n",
       " 'split5_train_score': array([0.97529967, 0.97529967, 0.78823102, 0.97529967, 0.75118053,\n",
       "        0.97766073, 0.7137668 , 0.97784235, 0.69088267, 0.97784235,\n",
       "        0.67598983, 0.97784235, 0.6594624 , 0.97784235, 0.64820196,\n",
       "        0.97784235, 0.63476208, 0.97784235, 0.6294951 , 0.97784235,\n",
       "        0.62549946, 0.97784235, 0.61605521, 0.97784235, 0.61278605,\n",
       "        0.97784235, 0.60806393, 0.97784235]),\n",
       " 'split6_train_score': array([0.97730162, 0.97730162, 0.78954058, 0.97693844, 0.75177047,\n",
       "        0.97802796, 0.71454512, 0.97857273, 0.69311785, 0.97857273,\n",
       "        0.67368803, 0.97857273, 0.65571091, 0.97875431, 0.65008171,\n",
       "        0.97875431, 0.63755221, 0.97875431, 0.62629381, 0.97875431,\n",
       "        0.61739604, 0.97875431, 0.61539858, 0.97875431, 0.60504812,\n",
       "        0.97875431, 0.60159797, 0.97875431]),\n",
       " 'split7_train_score': array([0.97458243, 0.97458243, 0.7899419 , 0.97494553, 0.75363108,\n",
       "        0.97639797, 0.71514161, 0.97676107, 0.68917938, 0.97676107,\n",
       "        0.67392883, 0.97676107, 0.65740741, 0.97694263, 0.64342774,\n",
       "        0.97694263, 0.63271605, 0.97694263, 0.6328976 , 0.97694263,\n",
       "        0.62418301, 0.97694263, 0.61528686, 0.97694263, 0.61401598,\n",
       "        0.97694263, 0.60566449, 0.97694263]),\n",
       " 'split8_train_score': array([0.97567617, 0.97567617, 0.79252133, 0.97476856, 0.75549101,\n",
       "        0.97676529, 0.71827918, 0.97658377, 0.69322926, 0.97658377,\n",
       "        0.67671084, 0.97676529, 0.66418588, 0.97676529, 0.65220548,\n",
       "        0.97676529, 0.64058813, 0.97676529, 0.62770013, 0.97676529,\n",
       "        0.61826103, 0.97676529, 0.61317844, 0.97676529, 0.60864041,\n",
       "        0.97676529, 0.60029043, 0.97676529]),\n",
       " 'split9_train_score': array([0.97186933, 0.97186933, 0.79056261, 0.97586207, 0.75099819,\n",
       "        0.97568058, 0.71161525, 0.97622505, 0.69147005, 0.976951  ,\n",
       "        0.67186933, 0.976951  , 0.6569873 , 0.97713249, 0.64446461,\n",
       "        0.97713249, 0.63575318, 0.97713249, 0.63194192, 0.97713249,\n",
       "        0.62250454, 0.97713249, 0.6154265 , 0.97713249, 0.60816697,\n",
       "        0.97713249, 0.60544465, 0.97713249]),\n",
       " 'mean_train_score': array([0.97540992, 0.97540992, 0.79043617, 0.97579108, 0.75396853,\n",
       "        0.97724406, 0.71566615, 0.97749831, 0.69182001, 0.97758908,\n",
       "        0.6748758 , 0.97764357, 0.66067398, 0.97773436, 0.64772441,\n",
       "        0.97773436, 0.63724541, 0.97773436, 0.63012594, 0.97773436,\n",
       "        0.62177195, 0.97773436, 0.61448887, 0.97773436, 0.60894955,\n",
       "        0.97773436, 0.60470026, 0.97773436]),\n",
       " 'std_train_score': array([0.00166871, 0.00166871, 0.00115283, 0.00093643, 0.00218542,\n",
       "        0.00089873, 0.00283446, 0.00085928, 0.00229874, 0.00079559,\n",
       "        0.00181052, 0.00083297, 0.00294327, 0.00080411, 0.00293343,\n",
       "        0.00080411, 0.0039194 , 0.00080411, 0.00367409, 0.00080411,\n",
       "        0.00340971, 0.00080411, 0.00275389, 0.00080411, 0.00356101,\n",
       "        0.00080411, 0.0031746 , 0.00080411])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "results_knn = pd.DataFrame(knn_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__n_neighbors</th>\n",
       "      <th>param_classifier__weights</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.118260</td>\n",
       "      <td>0.100887</td>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978016</td>\n",
       "      <td>0.976381</td>\n",
       "      <td>0.973665</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>0.977302</td>\n",
       "      <td>0.974582</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>0.971869</td>\n",
       "      <td>0.975410</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033625</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.063716</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978016</td>\n",
       "      <td>0.976381</td>\n",
       "      <td>0.973665</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>0.977302</td>\n",
       "      <td>0.974582</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>0.971869</td>\n",
       "      <td>0.975410</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036994</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.085726</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>0.576547</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790153</td>\n",
       "      <td>0.791061</td>\n",
       "      <td>0.791137</td>\n",
       "      <td>0.788231</td>\n",
       "      <td>0.789541</td>\n",
       "      <td>0.789942</td>\n",
       "      <td>0.792521</td>\n",
       "      <td>0.790563</td>\n",
       "      <td>0.790436</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035753</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.081808</td>\n",
       "      <td>0.011301</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977108</td>\n",
       "      <td>0.977108</td>\n",
       "      <td>0.974392</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>0.974946</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.975862</td>\n",
       "      <td>0.975791</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042096</td>\n",
       "      <td>0.011085</td>\n",
       "      <td>0.126321</td>\n",
       "      <td>0.033232</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.568404</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.578176</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754360</td>\n",
       "      <td>0.756177</td>\n",
       "      <td>0.758264</td>\n",
       "      <td>0.751181</td>\n",
       "      <td>0.751770</td>\n",
       "      <td>0.753631</td>\n",
       "      <td>0.755491</td>\n",
       "      <td>0.750998</td>\n",
       "      <td>0.753969</td>\n",
       "      <td>0.002185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045584</td>\n",
       "      <td>0.013124</td>\n",
       "      <td>0.104045</td>\n",
       "      <td>0.018449</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>0.605863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.978379</td>\n",
       "      <td>0.976934</td>\n",
       "      <td>0.977661</td>\n",
       "      <td>0.978028</td>\n",
       "      <td>0.976398</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.975681</td>\n",
       "      <td>0.977244</td>\n",
       "      <td>0.000899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.035224</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.101905</td>\n",
       "      <td>0.012146</td>\n",
       "      <td>4</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>0.568404</td>\n",
       "      <td>0.560261</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713118</td>\n",
       "      <td>0.715480</td>\n",
       "      <td>0.721032</td>\n",
       "      <td>0.713767</td>\n",
       "      <td>0.714545</td>\n",
       "      <td>0.715142</td>\n",
       "      <td>0.718279</td>\n",
       "      <td>0.711615</td>\n",
       "      <td>0.715666</td>\n",
       "      <td>0.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.039492</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>0.103177</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>4</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978924</td>\n",
       "      <td>0.978379</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978573</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.976584</td>\n",
       "      <td>0.976225</td>\n",
       "      <td>0.977498</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040933</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.135893</td>\n",
       "      <td>0.032437</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690225</td>\n",
       "      <td>0.689680</td>\n",
       "      <td>0.697058</td>\n",
       "      <td>0.690883</td>\n",
       "      <td>0.693118</td>\n",
       "      <td>0.689179</td>\n",
       "      <td>0.693229</td>\n",
       "      <td>0.691470</td>\n",
       "      <td>0.691820</td>\n",
       "      <td>0.002299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.053769</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>0.152615</td>\n",
       "      <td>0.048457</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978198</td>\n",
       "      <td>0.976934</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978573</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.976584</td>\n",
       "      <td>0.976951</td>\n",
       "      <td>0.977589</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.036910</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.116583</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>6</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.571661</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.542345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.674782</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>0.675990</td>\n",
       "      <td>0.673688</td>\n",
       "      <td>0.673929</td>\n",
       "      <td>0.676711</td>\n",
       "      <td>0.671869</td>\n",
       "      <td>0.674876</td>\n",
       "      <td>0.001811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.037091</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.121297</td>\n",
       "      <td>0.021461</td>\n",
       "      <td>6</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.976934</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978573</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.976951</td>\n",
       "      <td>0.977644</td>\n",
       "      <td>0.000833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.036689</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.125598</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>7</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.581433</td>\n",
       "      <td>0.548860</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.539088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664244</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.662913</td>\n",
       "      <td>0.659462</td>\n",
       "      <td>0.655711</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>0.664186</td>\n",
       "      <td>0.656987</td>\n",
       "      <td>0.660674</td>\n",
       "      <td>0.002943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.112036</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>7</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.605863</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.035991</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.126771</td>\n",
       "      <td>0.014511</td>\n",
       "      <td>8</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.570033</td>\n",
       "      <td>0.548860</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651344</td>\n",
       "      <td>0.646984</td>\n",
       "      <td>0.650018</td>\n",
       "      <td>0.648202</td>\n",
       "      <td>0.650082</td>\n",
       "      <td>0.643428</td>\n",
       "      <td>0.652205</td>\n",
       "      <td>0.644465</td>\n",
       "      <td>0.647724</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.036302</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>0.117926</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>8</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.607492</td>\n",
       "      <td>0.602606</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.047769</td>\n",
       "      <td>0.015279</td>\n",
       "      <td>0.150852</td>\n",
       "      <td>0.034955</td>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.576547</td>\n",
       "      <td>0.535831</td>\n",
       "      <td>0.566775</td>\n",
       "      <td>0.560261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640807</td>\n",
       "      <td>0.640988</td>\n",
       "      <td>0.641119</td>\n",
       "      <td>0.634762</td>\n",
       "      <td>0.637552</td>\n",
       "      <td>0.632716</td>\n",
       "      <td>0.640588</td>\n",
       "      <td>0.635753</td>\n",
       "      <td>0.637245</td>\n",
       "      <td>0.003919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.040263</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.147583</td>\n",
       "      <td>0.033172</td>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.614007</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.059473</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>0.218388</td>\n",
       "      <td>0.045183</td>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.574919</td>\n",
       "      <td>0.535831</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633903</td>\n",
       "      <td>0.631359</td>\n",
       "      <td>0.634399</td>\n",
       "      <td>0.629495</td>\n",
       "      <td>0.626294</td>\n",
       "      <td>0.632898</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.631942</td>\n",
       "      <td>0.630126</td>\n",
       "      <td>0.003674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.043981</td>\n",
       "      <td>0.016990</td>\n",
       "      <td>0.137926</td>\n",
       "      <td>0.029925</td>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>0.594463</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.587948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.038450</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.142228</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.581433</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625908</td>\n",
       "      <td>0.622820</td>\n",
       "      <td>0.622775</td>\n",
       "      <td>0.625499</td>\n",
       "      <td>0.617396</td>\n",
       "      <td>0.624183</td>\n",
       "      <td>0.618261</td>\n",
       "      <td>0.622505</td>\n",
       "      <td>0.621772</td>\n",
       "      <td>0.003410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.049059</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.154467</td>\n",
       "      <td>0.036574</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.033249</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.135237</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>12</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.573290</td>\n",
       "      <td>0.539088</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613372</td>\n",
       "      <td>0.616097</td>\n",
       "      <td>0.617327</td>\n",
       "      <td>0.616055</td>\n",
       "      <td>0.615399</td>\n",
       "      <td>0.615287</td>\n",
       "      <td>0.613178</td>\n",
       "      <td>0.615426</td>\n",
       "      <td>0.614489</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.033821</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.137937</td>\n",
       "      <td>0.012910</td>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.039026</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.153598</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>13</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.566775</td>\n",
       "      <td>0.524430</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.603561</td>\n",
       "      <td>0.613876</td>\n",
       "      <td>0.612786</td>\n",
       "      <td>0.605048</td>\n",
       "      <td>0.614016</td>\n",
       "      <td>0.608640</td>\n",
       "      <td>0.608167</td>\n",
       "      <td>0.608950</td>\n",
       "      <td>0.003561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.034895</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.140839</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>13</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>0.600977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.034982</td>\n",
       "      <td>0.005122</td>\n",
       "      <td>0.140094</td>\n",
       "      <td>0.013413</td>\n",
       "      <td>14</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.563518</td>\n",
       "      <td>0.530945</td>\n",
       "      <td>0.545603</td>\n",
       "      <td>0.530945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607740</td>\n",
       "      <td>0.603379</td>\n",
       "      <td>0.609880</td>\n",
       "      <td>0.608064</td>\n",
       "      <td>0.601598</td>\n",
       "      <td>0.605664</td>\n",
       "      <td>0.600290</td>\n",
       "      <td>0.605445</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.033819</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.148906</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>14</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.614007</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>0.599349</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.118260      0.100887         0.072642        0.013302   \n",
       "1        0.033625      0.002315         0.063716        0.005857   \n",
       "2        0.036994      0.007051         0.085726        0.010903   \n",
       "3        0.035753      0.009584         0.081808        0.011301   \n",
       "4        0.042096      0.011085         0.126321        0.033232   \n",
       "5        0.045584      0.013124         0.104045        0.018449   \n",
       "6        0.035224      0.004569         0.101905        0.012146   \n",
       "7        0.039492      0.013661         0.103177        0.021462   \n",
       "8        0.040933      0.010524         0.135893        0.032437   \n",
       "9        0.053769      0.018218         0.152615        0.048457   \n",
       "10       0.036910      0.004554         0.116583        0.018540   \n",
       "11       0.037091      0.005534         0.121297        0.021461   \n",
       "12       0.036689      0.005297         0.125598        0.019563   \n",
       "13       0.033482      0.002590         0.112036        0.004095   \n",
       "14       0.035991      0.005259         0.126771        0.014511   \n",
       "15       0.036302      0.007924         0.117926        0.005403   \n",
       "16       0.047769      0.015279         0.150852        0.034955   \n",
       "17       0.040263      0.007871         0.147583        0.033172   \n",
       "18       0.059473      0.018222         0.218388        0.045183   \n",
       "19       0.043981      0.016990         0.137926        0.029925   \n",
       "20       0.038450      0.010463         0.142228        0.011928   \n",
       "21       0.049059      0.018678         0.154467        0.036574   \n",
       "22       0.033249      0.001470         0.135237        0.007196   \n",
       "23       0.033821      0.001967         0.137937        0.012910   \n",
       "24       0.039026      0.005933         0.153598        0.017660   \n",
       "25       0.034895      0.002894         0.140839        0.007879   \n",
       "26       0.034982      0.005122         0.140094        0.013413   \n",
       "27       0.033819      0.001630         0.148906        0.025101   \n",
       "\n",
       "   param_classifier__n_neighbors param_classifier__weights  split0_test_score  \\\n",
       "0                              1                   uniform           0.587948   \n",
       "1                              1                  distance           0.587948   \n",
       "2                              2                   uniform           0.584691   \n",
       "3                              2                  distance           0.589577   \n",
       "4                              3                   uniform           0.568404   \n",
       "5                              3                  distance           0.596091   \n",
       "6                              4                   uniform           0.558632   \n",
       "7                              4                  distance           0.597720   \n",
       "8                              5                   uniform           0.561889   \n",
       "9                              5                  distance           0.597720   \n",
       "10                             6                   uniform           0.571661   \n",
       "11                             6                  distance           0.612378   \n",
       "12                             7                   uniform           0.581433   \n",
       "13                             7                  distance           0.605863   \n",
       "14                             8                   uniform           0.570033   \n",
       "15                             8                  distance           0.607492   \n",
       "16                             9                   uniform           0.576547   \n",
       "17                             9                  distance           0.614007   \n",
       "18                            10                   uniform           0.574919   \n",
       "19                            10                  distance           0.612378   \n",
       "20                            11                   uniform           0.581433   \n",
       "21                            11                  distance           0.612378   \n",
       "22                            12                   uniform           0.573290   \n",
       "23                            12                  distance           0.615635   \n",
       "24                            13                   uniform           0.566775   \n",
       "25                            13                  distance           0.615635   \n",
       "26                            14                   uniform           0.563518   \n",
       "27                            14                  distance           0.614007   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score       ...         \\\n",
       "0            0.592834           0.587948           0.591205       ...          \n",
       "1            0.592834           0.587948           0.591205       ...          \n",
       "2            0.565147           0.576547           0.592834       ...          \n",
       "3            0.597720           0.587948           0.584691       ...          \n",
       "4            0.552117           0.578176           0.561889       ...          \n",
       "5            0.597720           0.594463           0.605863       ...          \n",
       "6            0.568404           0.560261           0.565147       ...          \n",
       "7            0.600977           0.586319           0.596091       ...          \n",
       "8            0.561889           0.547231           0.547231       ...          \n",
       "9            0.600977           0.591205           0.591205       ...          \n",
       "10           0.543974           0.552117           0.542345       ...          \n",
       "11           0.596091           0.594463           0.600977       ...          \n",
       "12           0.548860           0.543974           0.539088       ...          \n",
       "13           0.597720           0.586319           0.594463       ...          \n",
       "14           0.548860           0.558632           0.550489       ...          \n",
       "15           0.602606           0.586319           0.584691       ...          \n",
       "16           0.535831           0.566775           0.560261       ...          \n",
       "17           0.591205           0.597720           0.596091       ...          \n",
       "18           0.535831           0.565147           0.550489       ...          \n",
       "19           0.594463           0.597720           0.587948       ...          \n",
       "20           0.543974           0.550489           0.558632       ...          \n",
       "21           0.589577           0.591205           0.600977       ...          \n",
       "22           0.539088           0.543974           0.547231       ...          \n",
       "23           0.592834           0.600977           0.596091       ...          \n",
       "24           0.524430           0.543974           0.543974       ...          \n",
       "25           0.586319           0.596091           0.600977       ...          \n",
       "26           0.530945           0.545603           0.530945       ...          \n",
       "27           0.584691           0.599349           0.589577       ...          \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.978016            0.976381            0.973665   \n",
       "1             0.978016            0.976381            0.973665   \n",
       "2             0.790153            0.791061            0.791137   \n",
       "3             0.977108            0.977108            0.974392   \n",
       "4             0.754360            0.756177            0.758264   \n",
       "5             0.978743            0.978379            0.976934   \n",
       "6             0.713118            0.715480            0.721032   \n",
       "7             0.978924            0.978379            0.977116   \n",
       "8             0.690225            0.689680            0.697058   \n",
       "9             0.979106            0.978198            0.976934   \n",
       "10            0.674419            0.674782            0.676171   \n",
       "11            0.979106            0.978743            0.976934   \n",
       "12            0.664244            0.662791            0.662913   \n",
       "13            0.979106            0.978743            0.977116   \n",
       "14            0.651344            0.646984            0.650018   \n",
       "15            0.979106            0.978743            0.977116   \n",
       "16            0.640807            0.640988            0.641119   \n",
       "17            0.979106            0.978743            0.977116   \n",
       "18            0.633903            0.631359            0.634399   \n",
       "19            0.979106            0.978743            0.977116   \n",
       "20            0.625908            0.622820            0.622775   \n",
       "21            0.979106            0.978743            0.977116   \n",
       "22            0.613372            0.616097            0.617327   \n",
       "23            0.979106            0.978743            0.977116   \n",
       "24            0.609375            0.603561            0.613876   \n",
       "25            0.979106            0.978743            0.977116   \n",
       "26            0.607740            0.603379            0.609880   \n",
       "27            0.979106            0.978743            0.977116   \n",
       "\n",
       "    split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0             0.975300            0.977302            0.974582   \n",
       "1             0.975300            0.977302            0.974582   \n",
       "2             0.788231            0.789541            0.789942   \n",
       "3             0.975300            0.976938            0.974946   \n",
       "4             0.751181            0.751770            0.753631   \n",
       "5             0.977661            0.978028            0.976398   \n",
       "6             0.713767            0.714545            0.715142   \n",
       "7             0.977842            0.978573            0.976761   \n",
       "8             0.690883            0.693118            0.689179   \n",
       "9             0.977842            0.978573            0.976761   \n",
       "10            0.675990            0.673688            0.673929   \n",
       "11            0.977842            0.978573            0.976761   \n",
       "12            0.659462            0.655711            0.657407   \n",
       "13            0.977842            0.978754            0.976943   \n",
       "14            0.648202            0.650082            0.643428   \n",
       "15            0.977842            0.978754            0.976943   \n",
       "16            0.634762            0.637552            0.632716   \n",
       "17            0.977842            0.978754            0.976943   \n",
       "18            0.629495            0.626294            0.632898   \n",
       "19            0.977842            0.978754            0.976943   \n",
       "20            0.625499            0.617396            0.624183   \n",
       "21            0.977842            0.978754            0.976943   \n",
       "22            0.616055            0.615399            0.615287   \n",
       "23            0.977842            0.978754            0.976943   \n",
       "24            0.612786            0.605048            0.614016   \n",
       "25            0.977842            0.978754            0.976943   \n",
       "26            0.608064            0.601598            0.605664   \n",
       "27            0.977842            0.978754            0.976943   \n",
       "\n",
       "    split8_train_score  split9_train_score  mean_train_score  std_train_score  \n",
       "0             0.975676            0.971869          0.975410         0.001669  \n",
       "1             0.975676            0.971869          0.975410         0.001669  \n",
       "2             0.792521            0.790563          0.790436         0.001153  \n",
       "3             0.974769            0.975862          0.975791         0.000936  \n",
       "4             0.755491            0.750998          0.753969         0.002185  \n",
       "5             0.976765            0.975681          0.977244         0.000899  \n",
       "6             0.718279            0.711615          0.715666         0.002834  \n",
       "7             0.976584            0.976225          0.977498         0.000859  \n",
       "8             0.693229            0.691470          0.691820         0.002299  \n",
       "9             0.976584            0.976951          0.977589         0.000796  \n",
       "10            0.676711            0.671869          0.674876         0.001811  \n",
       "11            0.976765            0.976951          0.977644         0.000833  \n",
       "12            0.664186            0.656987          0.660674         0.002943  \n",
       "13            0.976765            0.977132          0.977734         0.000804  \n",
       "14            0.652205            0.644465          0.647724         0.002933  \n",
       "15            0.976765            0.977132          0.977734         0.000804  \n",
       "16            0.640588            0.635753          0.637245         0.003919  \n",
       "17            0.976765            0.977132          0.977734         0.000804  \n",
       "18            0.627700            0.631942          0.630126         0.003674  \n",
       "19            0.976765            0.977132          0.977734         0.000804  \n",
       "20            0.618261            0.622505          0.621772         0.003410  \n",
       "21            0.976765            0.977132          0.977734         0.000804  \n",
       "22            0.613178            0.615426          0.614489         0.002754  \n",
       "23            0.976765            0.977132          0.977734         0.000804  \n",
       "24            0.608640            0.608167          0.608950         0.003561  \n",
       "25            0.976765            0.977132          0.977734         0.000804  \n",
       "26            0.600290            0.605445          0.604700         0.003175  \n",
       "27            0.976765            0.977132          0.977734         0.000804  \n",
       "\n",
       "[28 rows x 31 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_knn.head()\n",
    "results_knn.drop(['params'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'gini'}\n",
      "10-fold CV          : 0.5590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(10, shuffle=True, random_state=42)\n",
    "dt_grid = GridSearchCV(pipe, params, scoring='accuracy', cv=kfold, verbose=1, n_jobs=-1)\n",
    "dt_grid.fit(X, y)\n",
    "\n",
    "print(dt_grid.best_params_)\n",
    "\n",
    "val_col_space = 20\n",
    "print(\"{:{}}: {:.4f}\".format(\"10-fold CV\", val_col_space, dt_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/aaar2/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__criterion</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190236</td>\n",
       "      <td>0.018137</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.548860</td>\n",
       "      <td>0.576547</td>\n",
       "      <td>0.555375</td>\n",
       "      <td>0.535948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.389734</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.566775</td>\n",
       "      <td>0.550489</td>\n",
       "      <td>0.537459</td>\n",
       "      <td>0.568404</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979106</td>\n",
       "      <td>0.978743</td>\n",
       "      <td>0.977116</td>\n",
       "      <td>0.977842</td>\n",
       "      <td>0.978754</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.976765</td>\n",
       "      <td>0.977132</td>\n",
       "      <td>0.977734</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.190236      0.018137         0.001315        0.000286   \n",
       "1       0.389734      0.011747         0.001036        0.000051   \n",
       "\n",
       "  param_classifier__criterion  split0_test_score  split1_test_score  \\\n",
       "0                        gini           0.592834           0.548860   \n",
       "1                     entropy           0.566775           0.550489   \n",
       "\n",
       "   split2_test_score  split3_test_score  split4_test_score       ...         \\\n",
       "0           0.576547           0.555375           0.535948       ...          \n",
       "1           0.537459           0.568404           0.521242       ...          \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.979106            0.978743            0.977116   \n",
       "1            0.979106            0.978743            0.977116   \n",
       "\n",
       "   split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0            0.977842            0.978754            0.976943   \n",
       "1            0.977842            0.978754            0.976943   \n",
       "\n",
       "   split8_train_score  split9_train_score  mean_train_score  std_train_score  \n",
       "0            0.976765            0.977132          0.977734         0.000804  \n",
       "1            0.976765            0.977132          0.977734         0.000804  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dt = pd.DataFrame(dt_grid.cv_results_)\n",
    "results_dt.head()\n",
    "results_dt.drop(['params'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
